<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>ieat.models API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ieat.models</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L0-L554" class="git-link">Browse git</a>
</summary>
<pre><code class="python">from ieat.utils import resize, normalize_img, color_quantize_np

import os

import transformers
from transformers.modeling_gpt2 import GPT2LMHeadModel

import torch
import torch.nn as nn
from torch.nn.parameter import Parameter
import tensorflow as tf
import tensorflow_hub as hub

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import logging

os.environ[&#39;CUDA_LAUNCH_BLOCKING&#39;] = &#34;1&#34;
logger = logging.getLogger()

# # Code adapted from
# https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb
# - thanks to the author


class EmbeddingExtractor:
        &#34;&#34;&#34;Extracts embeddings from images with a pre-trained model.&#34;&#34;&#34;
        def __init__(self, model_name, from_cache):
                &#34;&#34;&#34;
                Parameters
                ----------
                model_name : str
                        A name for this model, used for caching.
                from_cache : bool
                        Whether to used cached embeddings.
                &#34;&#34;&#34;
                self.from_cache = from_cache
                self.model_name = model_name
                self.model = None

        def load_model(self):
                &#34;&#34;&#34;
                Loads the model, from the web or from the filesystem.
                &#34;&#34;&#34;
                raise NotImplementedError

        def extract_dir(self, d, file_types=(&#34;.jpg&#34;, &#34;.jpeg&#34;, &#34;.png&#34;, &#34;.webp&#34;), batch_size=None, visualize=False, **extract_params):
                &#34;&#34;&#34;
                Extracts embeddings from images in a directory.
                Parameters
                ----------
                d : str
                        path to a directory of images
                file_types : list[str]
                        list of acceptable file extensions for images
                batch_size : int
                        number of images processed at a time - helps when you have limited memory
                visualize : bool
                        whether to display the images after pre-processing
                extract_params : dict
                        additional parameters for extraction

                Returns
                -------
                encs : pd.DataFrame
                        a Pandas dataframe of features - see `EmbeddingExtractor.extract`
                &#34;&#34;&#34;
                embedding_path = self._make_embedding_path(d)
                image_paths = [
                        os.path.join(d, f) for f in os.listdir(d)
                        if os.path.splitext(f)[1] in file_types
                ]
                if self.from_cache and os.path.exists(embedding_path):
                        logger.info(&#34;Loading embeddings for %s from file&#34; % os.path.basename(d))
                        encs = pd.read_csv(embedding_path, index_col=0).set_index(&#34;img&#34;)
                        if visualize:
                                self.process_samples(image_paths, visualize=True)
                else:
                        logger.info(&#34;Extracting embeddings for %s&#34; % os.path.basename(d))
                        
                        # do extraction in batches to save memory
                        
                        encs = self.extract(
                                image_paths,
                                batch_size=batch_size,
                                output_path=embedding_path,
                                visualize=visualize,
                                **extract_params
                        )
                return encs

        def extract(self, image_paths, batch_size=None, output_path=None, gpu=False, visualize=False, **extract_kwargs):
                &#34;&#34;&#34;
                Extracts features from a set of image paths.

                Parameters
                ----------
                image_paths : str
                        a list of paths to images to extract features for
                batch_size : int or None
                        number of images processed at a time - helps when you have limited memory; if None, use just one batch
                output_path : str or None
                        path to save a CSV cache file with the extracted features; if none, don&#39;t cache
                gpu : bool
                        whether to use GPU (True) or CPU (False)
                visualize : bool
                        whether to display the images after pre-processing
                extract_kwargs : dict
                        additional parameters for extraction

                Returns
                -------
                encs : pd.DataFrame
                        data frame of features, indexed by the original image path
                &#34;&#34;&#34;
                if self.model is None:
                        self.load_model()
                if batch_size is None:
                        batch_size = len(image_paths)

                with torch.no_grad():  # saves some memory
                        batches = [image_paths[i:i+batch_size] for i in range(0, len(image_paths), batch_size)]

                        # model specific context extraction
                        encs = pd.concat([
                                pd.DataFrame(
                                        self._extract_context(self.process_samples(batch, visualize=visualize), gpu, **extract_kwargs)
                                )
                                for batch in batches
                        ])

                        encs[&#34;img&#34;] = [os.path.basename(path) for path in image_paths]

                        # DEPRECATED - NOW THAT CACHE IS STORED BY CATEGORY
                        # df[&#34;category&#34;] = [os.path.basename(os.path.dirname(path)) for path in image_paths]

                        if output_path is not None:
                                # add the image names to the CSV file
                                encs.to_csv(output_path)

                        return encs.set_index(&#34;img&#34;)

        def process_samples(self, image_paths, visualize=False):
                &#34;&#34;&#34;
                Pre-process the image samples for embedding extraction.

                Parameters
                ----------
                image_paths : list[str]
                        list of image paths to pre-process
                visualize : bool
                        whether to display the images after pre-processing

                Returns
                -------
                list
                        list of processed images, usually as `list[np.ndarray]`
                &#34;&#34;&#34;
                raise NotImplementedError

        def _extract_context(self, samples, gpu, **extract_kwargs) -&gt; np.ndarray:
                raise NotImplementedError

        def _make_embedding_path(self, d):
                return &#34;embeddings/{}_{}_{}_{}.csv&#34;.format(
                        os.path.basename(os.path.dirname(d)),
                        os.path.basename(d),
                        self.model_name,
                        self._make_param_path()
                )

        def _make_param_path(self):
                raise NotImplementedError

        @staticmethod
        def visualize(images, paths):
                &#34;&#34;&#34;
                Visualize some preprocessed images.

                Parameters
                ----------
                images : list[np.ndarray]
                        the images, as matrices
                paths : list[str]
                        list of the original image paths, so we can get the parent directory
                &#34;&#34;&#34;
                print(os.path.basename(os.path.dirname(paths[0])))
                f, axes = plt.subplots(1, len(images), dpi=300)
                for img, ax in zip(images, axes):
                        ax.axis(&#39;off&#39;)
                        ax.imshow(img)
                plt.show()


class SimCLRExtractor(EmbeddingExtractor):
        &#34;&#34;&#34;Extractor using the [SimCLR model](https://github.com/google-research/simclr).&#34;&#34;&#34;
        n_px = 224

        def __init__(self, model_name: str, depth: int, width: int, sk: int, **parent_params):
                &#34;&#34;&#34;
                Parameters
                ----------
                model_name : str
                        A name for this model, used for caching.
                depth : int
                        Depth of the ResNet used.
                width : int
                        Width of the resnet used.
                sk : bool
                        Whether to use selective kernels.
                parent_params
                &#34;&#34;&#34;
                super().__init__(model_name, **parent_params)
                tf.compat.v1.disable_eager_execution()
                self.depth = depth
                self.width = width
                self.sk = sk
                self.sess = None
                self.images = None

        def load_model(self):
                hub_path = f&#34;gs://simclr-checkpoints/simclrv2/pretrained/r{self.depth}_{self.width}x_sk{self.sk}/hub&#34;
                module = hub.Module(hub_path, trainable=False)
                self.images = tf.compat.v1.placeholder(tf.float32)
                self.model = module(inputs=self.images, signature=&#34;default&#34;, as_dict=True)
                self.sess = tf.compat.v1.Session()
                self.sess.run(tf.compat.v1.global_variables_initializer())

        def process_samples(self, image_paths: list, visualize=False):
                images = np.array([image/255 for image in resize(SimCLRExtractor.n_px, image_paths)])

                if visualize:
                        self.visualize(images, image_paths)

                return images

        def _extract_context(self, samples, gpu, **extract_kwargs) -&gt; np.ndarray:
                output = self.sess.run(self.model, {self.images: samples})
                # &#39;default&#39; is the representation output of the base ResNet network
                encs = output[&#39;default&#39;]
                return encs

        def _make_param_path(self):
                return f&#34;{self.depth}_{self.width}x_sk{self.sk}&#34;


class GPTExtractor(EmbeddingExtractor):
        &#34;&#34;&#34;Extractor using [iGPT](https://github.com/openai/image-gpt). You must download the model manually.&#34;&#34;&#34;
        MODELS = {&#34;l&#34;: (1536, 16, 48), &#34;m&#34;: (1024, 8, 36), &#34;s&#34;: (512, 8, 24)}

        def __init__(self, model_name, model_size, models_dir, color_clusters_dir, n_px, **parent_params):
                &#34;&#34;&#34;

                Parameters
                ----------
                model_name : str
                        A name for this model, used for caching.
                model_size : str
                        The size of iGPT used - &#34;s&#34; for small, &#34;m&#34; for medium, or &#34;l&#34; for large. The exact parameters are stored in
                        `GPTExtractor.MODELS`.
                models_dir : str
                        Path to directory with downloaded model. Make sure the params match the downloaded model.
                color_clusters_dir : str
                        Path to directory with the downloaded color clusters.
                n_px : int
                        The number of pixels used. All publicly available versions of iGPT are 32x32.
                parent_params
                &#34;&#34;&#34;
                super().__init__(model_name, **parent_params)

                self.n_px = n_px
                self.model_size = model_size

                color_clusters_file = &#34;%s/kmeans_centers.npy&#34; % color_clusters_dir
                self.clusters = np.load(color_clusters_file)  # get color clusters

                n_embd, n_head, n_layer = GPTExtractor.MODELS[model_size]  # set model hyperparameters

                self.vocab_size = len(self.clusters) + 1  # add one for start of sentence token

                self.config = transformers.GPT2Config(
                        vocab_size=self.vocab_size,
                        n_ctx=self.n_px * self.n_px,
                        n_positions=self.n_px * self.n_px,
                        n_embd=n_embd,
                        n_layer=n_layer,
                        n_head=n_head
                )
                self.model_path = &#34;%s/%s/model.ckpt-1000000.index&#34; % (models_dir, model_size)

        def _extract_context(self, samples, gpu, **extract_kwargs) -&gt; np.ndarray:
                raise NotImplementedError

        def load_model(self):
                assert os.path.exists(self.model_path), f&#34;There is no file at {self.model_path}&#34;
                self.model = ImageGPT2LMHeadModel.from_pretrained(
                        self.model_path, from_tf=True, config=self.config
                )

        def process_samples(self, image_paths, visualize=False):
                for path in image_paths:
                        assert os.path.exists(path), &#34;ERR: %s is not a valid path.&#34; % path
                # print(&#34;Num paths: %s&#34; % len(image_paths))
                x = resize(self.n_px, image_paths)
                # print(&#34;X shape: &#34;, x.shape)
                x_norm = normalize_img(x)  # normalize pixels values to -1 to +1
                samples = color_quantize_np(x_norm, self.clusters).reshape(
                        x_norm.shape[:-1])  # map pixels to closest color cluster

                if visualize:
                        samples_img = [
                                np.reshape(
                                        np.rint(127.5 * (self.clusters[s] + 1.0)), [self.n_px, self.n_px, 3]
                                ).astype(np.uint8) for s in samples
                        ]  # convert color clusters back to pixels
                        self.visualize(samples_img, image_paths)
                # print(&#34;Shape of samples: &#34;, samples.shape)
                return samples

        def _make_param_path(self):
                return &#34;{}_{}&#34;.format(
                        self.model_size,
                        self.n_px
                )

        def model_output(self, samples, gpu):
                &#34;&#34;&#34;
                Model output from every layer for a given input image.
                Embeddings can be extracted and aggregated from different layers (see the child classes).

                Parameters
                ----------
                samples : np.ndarray
                gpu : bool
                        whether to use GPU (True) or CPU (False)

                Returns
                -------
                output : tuple(torch.FloatTensor)
                        a Tensor of all hidden states
                &#34;&#34;&#34;
                context = np.concatenate(
                        (
                                np.full((samples.shape[0], 1), self.vocab_size - 1),
                                samples.reshape(-1, self.n_px * self.n_px),
                        ), axis=1
                )

                # must drop the last pixel to make room for the SOS
                context = torch.tensor(context[:, :-1]) if not gpu else torch.tensor(context[:, :-1]).cuda()
                return self.model(context, output_hidden_states=True, return_dict=True)


class LogitExtractor(GPTExtractor):
        &#34;&#34;&#34;Extractor for iGPT logit (projection head) layer.&#34;&#34;&#34;
        def _extract_context(self, samples, gpu, **extract_kwargs) -&gt; np.ndarray:
                output = self.model_output(samples, gpu)
                # just use the logit layer
                # extract the rep of the last input, as in sent-bias
                enc_last = output.logits[:, -1, :]

                return enc_last.numpy() if not gpu else enc_last.cpu().numpy()


class SENTExtractor(GPTExtractor):
        &#34;&#34;&#34;Extractor for last position of the last layer output.&#34;&#34;&#34;
        def _extract_context(self, samples, gpu, **extract_kwargs)  -&gt; np.ndarray:
                &#34;&#34;&#34;
                SENT uses the last hidden layer output.

                For details, see https://github.com/tanyichern/social-biases-contextualized/blob/master/gpt2.py.
                &#34;&#34;&#34;
                # initialize with SOS token
                output = self.model_output(samples, gpu)

                enc_last = output.hidden_states[-1][:, -1, :] # extract the rep of the last input

                return enc_last.numpy() if not gpu else enc_last.cpu().numpy()


class OpenAIExtractor(GPTExtractor):
        &#34;&#34;&#34;
        Pooled extraction method, used by the iGPT authors for linear evaluation.
        1. find $n^l = layer\_norm(h^l)$
        2. average pool across the sequence dimension:
        $$ f^l = \langle n^l_i \rangle_i $$
        &#34;&#34;&#34;
        def _extract_context(self, samples, gpu, **extract_kwargs) -&gt; np.ndarray:
                l = extract_kwargs.get(&#34;l&#34;, 20)

                output = self.model_output(samples, gpu)

                # extract the rep of the lth input
                h_l = output.hidden_states[l]
                norm = self.model.transformer.h[l+1].ln_1(h_l)
                enc = tf.reduce_mean(norm, axis=1)

                return enc.numpy() if not gpu else enc.cpu().numpy()


class ln_mod(nn.Module):
        &#34;&#34;&#34;
        Torch module for the iGPT modified linear head.
        From [apeguero1](https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb).
        &#34;&#34;&#34;
        def __init__(self, nx, eps=1e-5):
                super().__init__()
                self.eps = eps
                self.weight = Parameter(torch.Tensor(nx))

        def forward(self, x):  # input is not mean centered
                return x \
                        / torch.sqrt(torch.std(x, axis=-1, unbiased=False, keepdim=True) ** 2 + self.eps) \
                        * self.weight.data[..., :]


def load_tf_weights_in_image_gpt2(model, config, gpt2_checkpoint_path):
        &#34;&#34;&#34;
        Load tf checkpoints in a custom pytorch model.
        From [apeguero1](https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb).
        &#34;&#34;&#34;
        try:
                import re
                import tensorflow as tf
        except ImportError:
                logger.error(
                        &#34;Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see &#34;
                        &#34;https://www.tensorflow.org/install/ for installation instructions.&#34;
                )
                raise
        tf_path = os.path.abspath(gpt2_checkpoint_path)
        logger.debug(&#34;Converting TensorFlow checkpoint from {}&#34;.format(tf_path))
        # Load weights from TF model
        init_vars = tf.train.list_variables(tf_path)
        names = []
        arrays = []

        for name, shape in init_vars:
                logger.debug(&#34;Loading TF weight {} with shape {}&#34;.format(name, shape))
                array = tf.train.load_variable(tf_path, name)
                names.append(name)
                arrays.append(array.squeeze())

        for name, array in zip(names, arrays):
                name = name[6:]  # skip &#34;model/&#34;
                name = name.split(&#34;/&#34;)

                # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v
                # which are not required for using pretrained model
                if any(
                                n in [&#34;adam_v&#34;, &#34;adam_m&#34;, &#34;AdamWeightDecayOptimizer&#34;, &#34;AdamWeightDecayOptimizer_1&#34;, &#34;global_step&#34;]
                                for n in name
                ) or name[-1] in [&#39;_step&#39;]:
                        logger.debug(&#34;Skipping {}&#34;.format(&#34;/&#34;.join(name)))
                        continue

                pointer = model
                if name[-1] not in [&#34;wtet&#34;]:
                        pointer = getattr(pointer, &#34;transformer&#34;)

                for m_name in name:
                        if re.fullmatch(r&#34;[A-Za-z]+\d+&#34;, m_name):
                                scope_names = re.split(r&#34;(\d+)&#34;, m_name)
                        else:
                                scope_names = [m_name]

                        if scope_names[0] == &#34;w&#34; or scope_names[0] == &#34;g&#34;:
                                pointer = getattr(pointer, &#34;weight&#34;)
                        elif scope_names[0] == &#34;b&#34;:
                                pointer = getattr(pointer, &#34;bias&#34;)
                        elif scope_names[0] == &#34;wpe&#34; or scope_names[0] == &#34;wte&#34;:
                                pointer = getattr(pointer, scope_names[0])
                                pointer = getattr(pointer, &#34;weight&#34;)
                        elif scope_names[0] in [&#39;q_proj&#39;, &#39;k_proj&#39;, &#39;v_proj&#39;]:
                                pointer = getattr(pointer, &#39;c_attn&#39;)
                                pointer = getattr(pointer, &#39;weight&#39;)
                        elif len(name) == 3 and name[1] == &#34;attn&#34; and scope_names[0] == &#34;c_proj&#34;:
                                pointer = getattr(pointer, scope_names[0])
                                pointer = getattr(pointer, &#39;weight&#39;)
                        elif scope_names[0] == &#34;wtet&#34;:
                                pointer = getattr(pointer, &#34;lm_head&#34;)
                                pointer = getattr(pointer, &#39;weight&#39;)
                        elif scope_names[0] == &#34;sos&#34;:
                                pointer = getattr(pointer, &#34;wte&#34;)
                                pointer = getattr(pointer, &#39;weight&#39;)
                        else:
                                pointer = getattr(pointer, scope_names[0])
                        if len(scope_names) &gt;= 2:
                                num = int(scope_names[1])
                                pointer = pointer[num]

                if len(name) &gt; 1 and name[1] == &#34;attn&#34; or name[-1] == &#34;wtet&#34; or name[-1] == &#34;sos&#34; or name[-1] == &#34;wte&#34;:
                        pass  # array is used to initialize only part of the pointer so sizes won&#39;t match
                else:
                        try:
                                assert pointer.shape == array.shape
                        except AssertionError as e:
                                e.args += (pointer.shape, array.shape)
                                raise

                logger.debug(&#34;Initialize PyTorch weight {}&#34;.format(name))

                if name[-1] == &#34;q_proj&#34;:
                        pointer.data[:, :config.n_embd] = torch.from_numpy(array.reshape(config.n_embd, config.n_embd)).T
                elif name[-1] == &#34;k_proj&#34;:
                        pointer.data[:, config.n_embd:2 * config.n_embd] = torch.from_numpy(
                                array.reshape(config.n_embd, config.n_embd)).T
                elif name[-1] == &#34;v_proj&#34;:
                        pointer.data[:, 2 * config.n_embd:] = torch.from_numpy(array.reshape(config.n_embd, config.n_embd)).T
                elif len(name) == 3 and name[1] == &#34;attn&#34; and name[2] == &#34;c_proj&#34;:
                        pointer.data = torch.from_numpy(array.reshape(config.n_embd, config.n_embd))
                elif name[-1] == &#34;wtet&#34;:
                        pointer.data = torch.from_numpy(array)
                elif name[-1] == &#34;wte&#34;:
                        pointer.data[:config.vocab_size - 1, :] = torch.from_numpy(array)
                elif name[-1] == &#34;sos&#34;:
                        pointer.data[-1] = torch.from_numpy(array)
                else:
                        pointer.data = torch.from_numpy(array)

        return model


def replace_ln(m, name, config):
        for attr_str in dir(m):
                target_attr = getattr(m, attr_str)
                if type(target_attr) == torch.nn.LayerNorm:
                        setattr(m, attr_str, ln_mod(config.n_embd, config.layer_norm_epsilon))

        for n, ch in m.named_children():
                replace_ln(ch, n, config)


class ImageGPT2LMHeadModel(GPT2LMHeadModel):
        &#34;&#34;&#34;
        Extension of the HuggingFace `GPT2LMHeadModel` for iGPT.
        From [apeguero1](https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb).
        &#34;&#34;&#34;
        load_tf_weights = load_tf_weights_in_image_gpt2

        def __init__(self, config):
                super().__init__(config)
                self.lm_head = nn.Linear(config.n_embd, config.vocab_size - 1, bias=False)
                replace_ln(self, &#34;net&#34;, config)  # replace layer normalization
                for n in range(config.n_layer):
                        self.transformer.h[n].mlp.act = ImageGPT2LMHeadModel.gelu2  # replace activation

        def tie_weights(self):  # image-gpt doesn&#39;t tie output and input embeddings
                pass

        @staticmethod
        def gelu2(x):
                return x * torch.sigmoid(1.702 * x)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ieat.models.load_tf_weights_in_image_gpt2"><code class="name flex">
<span>def <span class="ident">load_tf_weights_in_image_gpt2</span></span>(<span>model, config, gpt2_checkpoint_path)</span>
</code></dt>
<dd>
<div class="desc"><p>Load tf checkpoints in a custom pytorch model.
From <a href="https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb">apeguero1</a>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L419-L523" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def load_tf_weights_in_image_gpt2(model, config, gpt2_checkpoint_path):
        &#34;&#34;&#34;
        Load tf checkpoints in a custom pytorch model.
        From [apeguero1](https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb).
        &#34;&#34;&#34;
        try:
                import re
                import tensorflow as tf
        except ImportError:
                logger.error(
                        &#34;Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see &#34;
                        &#34;https://www.tensorflow.org/install/ for installation instructions.&#34;
                )
                raise
        tf_path = os.path.abspath(gpt2_checkpoint_path)
        logger.debug(&#34;Converting TensorFlow checkpoint from {}&#34;.format(tf_path))
        # Load weights from TF model
        init_vars = tf.train.list_variables(tf_path)
        names = []
        arrays = []

        for name, shape in init_vars:
                logger.debug(&#34;Loading TF weight {} with shape {}&#34;.format(name, shape))
                array = tf.train.load_variable(tf_path, name)
                names.append(name)
                arrays.append(array.squeeze())

        for name, array in zip(names, arrays):
                name = name[6:]  # skip &#34;model/&#34;
                name = name.split(&#34;/&#34;)

                # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v
                # which are not required for using pretrained model
                if any(
                                n in [&#34;adam_v&#34;, &#34;adam_m&#34;, &#34;AdamWeightDecayOptimizer&#34;, &#34;AdamWeightDecayOptimizer_1&#34;, &#34;global_step&#34;]
                                for n in name
                ) or name[-1] in [&#39;_step&#39;]:
                        logger.debug(&#34;Skipping {}&#34;.format(&#34;/&#34;.join(name)))
                        continue

                pointer = model
                if name[-1] not in [&#34;wtet&#34;]:
                        pointer = getattr(pointer, &#34;transformer&#34;)

                for m_name in name:
                        if re.fullmatch(r&#34;[A-Za-z]+\d+&#34;, m_name):
                                scope_names = re.split(r&#34;(\d+)&#34;, m_name)
                        else:
                                scope_names = [m_name]

                        if scope_names[0] == &#34;w&#34; or scope_names[0] == &#34;g&#34;:
                                pointer = getattr(pointer, &#34;weight&#34;)
                        elif scope_names[0] == &#34;b&#34;:
                                pointer = getattr(pointer, &#34;bias&#34;)
                        elif scope_names[0] == &#34;wpe&#34; or scope_names[0] == &#34;wte&#34;:
                                pointer = getattr(pointer, scope_names[0])
                                pointer = getattr(pointer, &#34;weight&#34;)
                        elif scope_names[0] in [&#39;q_proj&#39;, &#39;k_proj&#39;, &#39;v_proj&#39;]:
                                pointer = getattr(pointer, &#39;c_attn&#39;)
                                pointer = getattr(pointer, &#39;weight&#39;)
                        elif len(name) == 3 and name[1] == &#34;attn&#34; and scope_names[0] == &#34;c_proj&#34;:
                                pointer = getattr(pointer, scope_names[0])
                                pointer = getattr(pointer, &#39;weight&#39;)
                        elif scope_names[0] == &#34;wtet&#34;:
                                pointer = getattr(pointer, &#34;lm_head&#34;)
                                pointer = getattr(pointer, &#39;weight&#39;)
                        elif scope_names[0] == &#34;sos&#34;:
                                pointer = getattr(pointer, &#34;wte&#34;)
                                pointer = getattr(pointer, &#39;weight&#39;)
                        else:
                                pointer = getattr(pointer, scope_names[0])
                        if len(scope_names) &gt;= 2:
                                num = int(scope_names[1])
                                pointer = pointer[num]

                if len(name) &gt; 1 and name[1] == &#34;attn&#34; or name[-1] == &#34;wtet&#34; or name[-1] == &#34;sos&#34; or name[-1] == &#34;wte&#34;:
                        pass  # array is used to initialize only part of the pointer so sizes won&#39;t match
                else:
                        try:
                                assert pointer.shape == array.shape
                        except AssertionError as e:
                                e.args += (pointer.shape, array.shape)
                                raise

                logger.debug(&#34;Initialize PyTorch weight {}&#34;.format(name))

                if name[-1] == &#34;q_proj&#34;:
                        pointer.data[:, :config.n_embd] = torch.from_numpy(array.reshape(config.n_embd, config.n_embd)).T
                elif name[-1] == &#34;k_proj&#34;:
                        pointer.data[:, config.n_embd:2 * config.n_embd] = torch.from_numpy(
                                array.reshape(config.n_embd, config.n_embd)).T
                elif name[-1] == &#34;v_proj&#34;:
                        pointer.data[:, 2 * config.n_embd:] = torch.from_numpy(array.reshape(config.n_embd, config.n_embd)).T
                elif len(name) == 3 and name[1] == &#34;attn&#34; and name[2] == &#34;c_proj&#34;:
                        pointer.data = torch.from_numpy(array.reshape(config.n_embd, config.n_embd))
                elif name[-1] == &#34;wtet&#34;:
                        pointer.data = torch.from_numpy(array)
                elif name[-1] == &#34;wte&#34;:
                        pointer.data[:config.vocab_size - 1, :] = torch.from_numpy(array)
                elif name[-1] == &#34;sos&#34;:
                        pointer.data[-1] = torch.from_numpy(array)
                else:
                        pointer.data = torch.from_numpy(array)

        return model</code></pre>
</details>
</dd>
<dt id="ieat.models.replace_ln"><code class="name flex">
<span>def <span class="ident">replace_ln</span></span>(<span>m, name, config)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L526-L533" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def replace_ln(m, name, config):
        for attr_str in dir(m):
                target_attr = getattr(m, attr_str)
                if type(target_attr) == torch.nn.LayerNorm:
                        setattr(m, attr_str, ln_mod(config.n_embd, config.layer_norm_epsilon))

        for n, ch in m.named_children():
                replace_ln(ch, n, config)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ieat.models.EmbeddingExtractor"><code class="flex name class">
<span>class <span class="ident">EmbeddingExtractor</span></span>
<span>(</span><span>model_name, from_cache)</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts embeddings from images with a pre-trained model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_name</code></strong> :&ensp;<code>str</code></dt>
<dd>A name for this model, used for caching.</dd>
<dt><strong><code>from_cache</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to used cached embeddings.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L28-L194" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class EmbeddingExtractor:
        &#34;&#34;&#34;Extracts embeddings from images with a pre-trained model.&#34;&#34;&#34;
        def __init__(self, model_name, from_cache):
                &#34;&#34;&#34;
                Parameters
                ----------
                model_name : str
                        A name for this model, used for caching.
                from_cache : bool
                        Whether to used cached embeddings.
                &#34;&#34;&#34;
                self.from_cache = from_cache
                self.model_name = model_name
                self.model = None

        def load_model(self):
                &#34;&#34;&#34;
                Loads the model, from the web or from the filesystem.
                &#34;&#34;&#34;
                raise NotImplementedError

        def extract_dir(self, d, file_types=(&#34;.jpg&#34;, &#34;.jpeg&#34;, &#34;.png&#34;, &#34;.webp&#34;), batch_size=None, visualize=False, **extract_params):
                &#34;&#34;&#34;
                Extracts embeddings from images in a directory.
                Parameters
                ----------
                d : str
                        path to a directory of images
                file_types : list[str]
                        list of acceptable file extensions for images
                batch_size : int
                        number of images processed at a time - helps when you have limited memory
                visualize : bool
                        whether to display the images after pre-processing
                extract_params : dict
                        additional parameters for extraction

                Returns
                -------
                encs : pd.DataFrame
                        a Pandas dataframe of features - see `EmbeddingExtractor.extract`
                &#34;&#34;&#34;
                embedding_path = self._make_embedding_path(d)
                image_paths = [
                        os.path.join(d, f) for f in os.listdir(d)
                        if os.path.splitext(f)[1] in file_types
                ]
                if self.from_cache and os.path.exists(embedding_path):
                        logger.info(&#34;Loading embeddings for %s from file&#34; % os.path.basename(d))
                        encs = pd.read_csv(embedding_path, index_col=0).set_index(&#34;img&#34;)
                        if visualize:
                                self.process_samples(image_paths, visualize=True)
                else:
                        logger.info(&#34;Extracting embeddings for %s&#34; % os.path.basename(d))
                        
                        # do extraction in batches to save memory
                        
                        encs = self.extract(
                                image_paths,
                                batch_size=batch_size,
                                output_path=embedding_path,
                                visualize=visualize,
                                **extract_params
                        )
                return encs

        def extract(self, image_paths, batch_size=None, output_path=None, gpu=False, visualize=False, **extract_kwargs):
                &#34;&#34;&#34;
                Extracts features from a set of image paths.

                Parameters
                ----------
                image_paths : str
                        a list of paths to images to extract features for
                batch_size : int or None
                        number of images processed at a time - helps when you have limited memory; if None, use just one batch
                output_path : str or None
                        path to save a CSV cache file with the extracted features; if none, don&#39;t cache
                gpu : bool
                        whether to use GPU (True) or CPU (False)
                visualize : bool
                        whether to display the images after pre-processing
                extract_kwargs : dict
                        additional parameters for extraction

                Returns
                -------
                encs : pd.DataFrame
                        data frame of features, indexed by the original image path
                &#34;&#34;&#34;
                if self.model is None:
                        self.load_model()
                if batch_size is None:
                        batch_size = len(image_paths)

                with torch.no_grad():  # saves some memory
                        batches = [image_paths[i:i+batch_size] for i in range(0, len(image_paths), batch_size)]

                        # model specific context extraction
                        encs = pd.concat([
                                pd.DataFrame(
                                        self._extract_context(self.process_samples(batch, visualize=visualize), gpu, **extract_kwargs)
                                )
                                for batch in batches
                        ])

                        encs[&#34;img&#34;] = [os.path.basename(path) for path in image_paths]

                        # DEPRECATED - NOW THAT CACHE IS STORED BY CATEGORY
                        # df[&#34;category&#34;] = [os.path.basename(os.path.dirname(path)) for path in image_paths]

                        if output_path is not None:
                                # add the image names to the CSV file
                                encs.to_csv(output_path)

                        return encs.set_index(&#34;img&#34;)

        def process_samples(self, image_paths, visualize=False):
                &#34;&#34;&#34;
                Pre-process the image samples for embedding extraction.

                Parameters
                ----------
                image_paths : list[str]
                        list of image paths to pre-process
                visualize : bool
                        whether to display the images after pre-processing

                Returns
                -------
                list
                        list of processed images, usually as `list[np.ndarray]`
                &#34;&#34;&#34;
                raise NotImplementedError

        def _extract_context(self, samples, gpu, **extract_kwargs) -&gt; np.ndarray:
                raise NotImplementedError

        def _make_embedding_path(self, d):
                return &#34;embeddings/{}_{}_{}_{}.csv&#34;.format(
                        os.path.basename(os.path.dirname(d)),
                        os.path.basename(d),
                        self.model_name,
                        self._make_param_path()
                )

        def _make_param_path(self):
                raise NotImplementedError

        @staticmethod
        def visualize(images, paths):
                &#34;&#34;&#34;
                Visualize some preprocessed images.

                Parameters
                ----------
                images : list[np.ndarray]
                        the images, as matrices
                paths : list[str]
                        list of the original image paths, so we can get the parent directory
                &#34;&#34;&#34;
                print(os.path.basename(os.path.dirname(paths[0])))
                f, axes = plt.subplots(1, len(images), dpi=300)
                for img, ax in zip(images, axes):
                        ax.axis(&#39;off&#39;)
                        ax.imshow(img)
                plt.show()</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="ieat.models.GPTExtractor" href="#ieat.models.GPTExtractor">GPTExtractor</a></li>
<li><a title="ieat.models.SimCLRExtractor" href="#ieat.models.SimCLRExtractor">SimCLRExtractor</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="ieat.models.EmbeddingExtractor.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>images, paths)</span>
</code></dt>
<dd>
<div class="desc"><p>Visualize some preprocessed images.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>images</code></strong> :&ensp;<code>list[np.ndarray]</code></dt>
<dd>the images, as matrices</dd>
<dt><strong><code>paths</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>list of the original image paths, so we can get the parent directory</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L177-L194" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@staticmethod
def visualize(images, paths):
        &#34;&#34;&#34;
        Visualize some preprocessed images.

        Parameters
        ----------
        images : list[np.ndarray]
                the images, as matrices
        paths : list[str]
                list of the original image paths, so we can get the parent directory
        &#34;&#34;&#34;
        print(os.path.basename(os.path.dirname(paths[0])))
        f, axes = plt.subplots(1, len(images), dpi=300)
        for img, ax in zip(images, axes):
                ax.axis(&#39;off&#39;)
                ax.imshow(img)
        plt.show()</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ieat.models.EmbeddingExtractor.extract"><code class="name flex">
<span>def <span class="ident">extract</span></span>(<span>self, image_paths, batch_size=None, output_path=None, gpu=False, visualize=False, **extract_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts features from a set of image paths.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>image_paths</code></strong> :&ensp;<code>str</code></dt>
<dd>a list of paths to images to extract features for</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code> or <code>None</code></dt>
<dd>number of images processed at a time - helps when you have limited memory; if None, use just one batch</dd>
<dt><strong><code>output_path</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>path to save a CSV cache file with the extracted features; if none, don't cache</dd>
<dt><strong><code>gpu</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to use GPU (True) or CPU (False)</dd>
<dt><strong><code>visualize</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to display the images after pre-processing</dd>
<dt><strong><code>extract_kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>additional parameters for extraction</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>encs</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>data frame of features, indexed by the original image path</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L94-L143" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def extract(self, image_paths, batch_size=None, output_path=None, gpu=False, visualize=False, **extract_kwargs):
        &#34;&#34;&#34;
        Extracts features from a set of image paths.

        Parameters
        ----------
        image_paths : str
                a list of paths to images to extract features for
        batch_size : int or None
                number of images processed at a time - helps when you have limited memory; if None, use just one batch
        output_path : str or None
                path to save a CSV cache file with the extracted features; if none, don&#39;t cache
        gpu : bool
                whether to use GPU (True) or CPU (False)
        visualize : bool
                whether to display the images after pre-processing
        extract_kwargs : dict
                additional parameters for extraction

        Returns
        -------
        encs : pd.DataFrame
                data frame of features, indexed by the original image path
        &#34;&#34;&#34;
        if self.model is None:
                self.load_model()
        if batch_size is None:
                batch_size = len(image_paths)

        with torch.no_grad():  # saves some memory
                batches = [image_paths[i:i+batch_size] for i in range(0, len(image_paths), batch_size)]

                # model specific context extraction
                encs = pd.concat([
                        pd.DataFrame(
                                self._extract_context(self.process_samples(batch, visualize=visualize), gpu, **extract_kwargs)
                        )
                        for batch in batches
                ])

                encs[&#34;img&#34;] = [os.path.basename(path) for path in image_paths]

                # DEPRECATED - NOW THAT CACHE IS STORED BY CATEGORY
                # df[&#34;category&#34;] = [os.path.basename(os.path.dirname(path)) for path in image_paths]

                if output_path is not None:
                        # add the image names to the CSV file
                        encs.to_csv(output_path)

                return encs.set_index(&#34;img&#34;)</code></pre>
</details>
</dd>
<dt id="ieat.models.EmbeddingExtractor.extract_dir"><code class="name flex">
<span>def <span class="ident">extract_dir</span></span>(<span>self, d, file_types=('.jpg', '.jpeg', '.png', '.webp'), batch_size=None, visualize=False, **extract_params)</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts embeddings from images in a directory.
Parameters</p>
<hr>
<dl>
<dt><strong><code>d</code></strong> :&ensp;<code>str</code></dt>
<dd>path to a directory of images</dd>
<dt><strong><code>file_types</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>list of acceptable file extensions for images</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>number of images processed at a time - helps when you have limited memory</dd>
<dt><strong><code>visualize</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to display the images after pre-processing</dd>
<dt><strong><code>extract_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>additional parameters for extraction</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>encs</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>a Pandas dataframe of features - see <code><a title="ieat.models.EmbeddingExtractor.extract" href="#ieat.models.EmbeddingExtractor.extract">EmbeddingExtractor.extract()</a></code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L49-L92" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def extract_dir(self, d, file_types=(&#34;.jpg&#34;, &#34;.jpeg&#34;, &#34;.png&#34;, &#34;.webp&#34;), batch_size=None, visualize=False, **extract_params):
        &#34;&#34;&#34;
        Extracts embeddings from images in a directory.
        Parameters
        ----------
        d : str
                path to a directory of images
        file_types : list[str]
                list of acceptable file extensions for images
        batch_size : int
                number of images processed at a time - helps when you have limited memory
        visualize : bool
                whether to display the images after pre-processing
        extract_params : dict
                additional parameters for extraction

        Returns
        -------
        encs : pd.DataFrame
                a Pandas dataframe of features - see `EmbeddingExtractor.extract`
        &#34;&#34;&#34;
        embedding_path = self._make_embedding_path(d)
        image_paths = [
                os.path.join(d, f) for f in os.listdir(d)
                if os.path.splitext(f)[1] in file_types
        ]
        if self.from_cache and os.path.exists(embedding_path):
                logger.info(&#34;Loading embeddings for %s from file&#34; % os.path.basename(d))
                encs = pd.read_csv(embedding_path, index_col=0).set_index(&#34;img&#34;)
                if visualize:
                        self.process_samples(image_paths, visualize=True)
        else:
                logger.info(&#34;Extracting embeddings for %s&#34; % os.path.basename(d))
                
                # do extraction in batches to save memory
                
                encs = self.extract(
                        image_paths,
                        batch_size=batch_size,
                        output_path=embedding_path,
                        visualize=visualize,
                        **extract_params
                )
        return encs</code></pre>
</details>
</dd>
<dt id="ieat.models.EmbeddingExtractor.load_model"><code class="name flex">
<span>def <span class="ident">load_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads the model, from the web or from the filesystem.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L43-L47" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def load_model(self):
        &#34;&#34;&#34;
        Loads the model, from the web or from the filesystem.
        &#34;&#34;&#34;
        raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="ieat.models.EmbeddingExtractor.process_samples"><code class="name flex">
<span>def <span class="ident">process_samples</span></span>(<span>self, image_paths, visualize=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Pre-process the image samples for embedding extraction.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>image_paths</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>list of image paths to pre-process</dd>
<dt><strong><code>visualize</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to display the images after pre-processing</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>list of processed images, usually as <code>list[np.ndarray]</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L145-L161" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def process_samples(self, image_paths, visualize=False):
        &#34;&#34;&#34;
        Pre-process the image samples for embedding extraction.

        Parameters
        ----------
        image_paths : list[str]
                list of image paths to pre-process
        visualize : bool
                whether to display the images after pre-processing

        Returns
        -------
        list
                list of processed images, usually as `list[np.ndarray]`
        &#34;&#34;&#34;
        raise NotImplementedError</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ieat.models.GPTExtractor"><code class="flex name class">
<span>class <span class="ident">GPTExtractor</span></span>
<span>(</span><span>model_name, model_size, models_dir, color_clusters_dir, n_px, **parent_params)</span>
</code></dt>
<dd>
<div class="desc"><p>Extractor using <a href="https://github.com/openai/image-gpt">iGPT</a>. You must download the model manually.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_name</code></strong> :&ensp;<code>str</code></dt>
<dd>A name for this model, used for caching.</dd>
<dt><strong><code>model_size</code></strong> :&ensp;<code>str</code></dt>
<dd>The size of iGPT used - "s" for small, "m" for medium, or "l" for large. The exact parameters are stored in
<code><a title="ieat.models.GPTExtractor.MODELS" href="#ieat.models.GPTExtractor.MODELS">GPTExtractor.MODELS</a></code>.</dd>
<dt><strong><code>models_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to directory with downloaded model. Make sure the params match the downloaded model.</dd>
<dt><strong><code>color_clusters_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to directory with the downloaded color clusters.</dd>
<dt><strong><code>n_px</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of pixels used. All publicly available versions of iGPT are 32x32.</dd>
<dt><strong><code>parent_params</code></strong></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L249-L353" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class GPTExtractor(EmbeddingExtractor):
        &#34;&#34;&#34;Extractor using [iGPT](https://github.com/openai/image-gpt). You must download the model manually.&#34;&#34;&#34;
        MODELS = {&#34;l&#34;: (1536, 16, 48), &#34;m&#34;: (1024, 8, 36), &#34;s&#34;: (512, 8, 24)}

        def __init__(self, model_name, model_size, models_dir, color_clusters_dir, n_px, **parent_params):
                &#34;&#34;&#34;

                Parameters
                ----------
                model_name : str
                        A name for this model, used for caching.
                model_size : str
                        The size of iGPT used - &#34;s&#34; for small, &#34;m&#34; for medium, or &#34;l&#34; for large. The exact parameters are stored in
                        `GPTExtractor.MODELS`.
                models_dir : str
                        Path to directory with downloaded model. Make sure the params match the downloaded model.
                color_clusters_dir : str
                        Path to directory with the downloaded color clusters.
                n_px : int
                        The number of pixels used. All publicly available versions of iGPT are 32x32.
                parent_params
                &#34;&#34;&#34;
                super().__init__(model_name, **parent_params)

                self.n_px = n_px
                self.model_size = model_size

                color_clusters_file = &#34;%s/kmeans_centers.npy&#34; % color_clusters_dir
                self.clusters = np.load(color_clusters_file)  # get color clusters

                n_embd, n_head, n_layer = GPTExtractor.MODELS[model_size]  # set model hyperparameters

                self.vocab_size = len(self.clusters) + 1  # add one for start of sentence token

                self.config = transformers.GPT2Config(
                        vocab_size=self.vocab_size,
                        n_ctx=self.n_px * self.n_px,
                        n_positions=self.n_px * self.n_px,
                        n_embd=n_embd,
                        n_layer=n_layer,
                        n_head=n_head
                )
                self.model_path = &#34;%s/%s/model.ckpt-1000000.index&#34; % (models_dir, model_size)

        def _extract_context(self, samples, gpu, **extract_kwargs) -&gt; np.ndarray:
                raise NotImplementedError

        def load_model(self):
                assert os.path.exists(self.model_path), f&#34;There is no file at {self.model_path}&#34;
                self.model = ImageGPT2LMHeadModel.from_pretrained(
                        self.model_path, from_tf=True, config=self.config
                )

        def process_samples(self, image_paths, visualize=False):
                for path in image_paths:
                        assert os.path.exists(path), &#34;ERR: %s is not a valid path.&#34; % path
                # print(&#34;Num paths: %s&#34; % len(image_paths))
                x = resize(self.n_px, image_paths)
                # print(&#34;X shape: &#34;, x.shape)
                x_norm = normalize_img(x)  # normalize pixels values to -1 to +1
                samples = color_quantize_np(x_norm, self.clusters).reshape(
                        x_norm.shape[:-1])  # map pixels to closest color cluster

                if visualize:
                        samples_img = [
                                np.reshape(
                                        np.rint(127.5 * (self.clusters[s] + 1.0)), [self.n_px, self.n_px, 3]
                                ).astype(np.uint8) for s in samples
                        ]  # convert color clusters back to pixels
                        self.visualize(samples_img, image_paths)
                # print(&#34;Shape of samples: &#34;, samples.shape)
                return samples

        def _make_param_path(self):
                return &#34;{}_{}&#34;.format(
                        self.model_size,
                        self.n_px
                )

        def model_output(self, samples, gpu):
                &#34;&#34;&#34;
                Model output from every layer for a given input image.
                Embeddings can be extracted and aggregated from different layers (see the child classes).

                Parameters
                ----------
                samples : np.ndarray
                gpu : bool
                        whether to use GPU (True) or CPU (False)

                Returns
                -------
                output : tuple(torch.FloatTensor)
                        a Tensor of all hidden states
                &#34;&#34;&#34;
                context = np.concatenate(
                        (
                                np.full((samples.shape[0], 1), self.vocab_size - 1),
                                samples.reshape(-1, self.n_px * self.n_px),
                        ), axis=1
                )

                # must drop the last pixel to make room for the SOS
                context = torch.tensor(context[:, :-1]) if not gpu else torch.tensor(context[:, :-1]).cuda()
                return self.model(context, output_hidden_states=True, return_dict=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ieat.models.EmbeddingExtractor" href="#ieat.models.EmbeddingExtractor">EmbeddingExtractor</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="ieat.models.LogitExtractor" href="#ieat.models.LogitExtractor">LogitExtractor</a></li>
<li><a title="ieat.models.OpenAIExtractor" href="#ieat.models.OpenAIExtractor">OpenAIExtractor</a></li>
<li><a title="ieat.models.SENTExtractor" href="#ieat.models.SENTExtractor">SENTExtractor</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="ieat.models.GPTExtractor.MODELS"><code class="name">var <span class="ident">MODELS</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ieat.models.GPTExtractor.model_output"><code class="name flex">
<span>def <span class="ident">model_output</span></span>(<span>self, samples, gpu)</span>
</code></dt>
<dd>
<div class="desc"><p>Model output from every layer for a given input image.
Embeddings can be extracted and aggregated from different layers (see the child classes).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>samples</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>gpu</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to use GPU (True) or CPU (False)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>output</code></strong> :&ensp;<code>tuple(torch.FloatTensor)</code></dt>
<dd>a Tensor of all hidden states</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L328-L353" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def model_output(self, samples, gpu):
        &#34;&#34;&#34;
        Model output from every layer for a given input image.
        Embeddings can be extracted and aggregated from different layers (see the child classes).

        Parameters
        ----------
        samples : np.ndarray
        gpu : bool
                whether to use GPU (True) or CPU (False)

        Returns
        -------
        output : tuple(torch.FloatTensor)
                a Tensor of all hidden states
        &#34;&#34;&#34;
        context = np.concatenate(
                (
                        np.full((samples.shape[0], 1), self.vocab_size - 1),
                        samples.reshape(-1, self.n_px * self.n_px),
                ), axis=1
        )

        # must drop the last pixel to make room for the SOS
        context = torch.tensor(context[:, :-1]) if not gpu else torch.tensor(context[:, :-1]).cuda()
        return self.model(context, output_hidden_states=True, return_dict=True)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ieat.models.EmbeddingExtractor" href="#ieat.models.EmbeddingExtractor">EmbeddingExtractor</a></b></code>:
<ul class="hlist">
<li><code><a title="ieat.models.EmbeddingExtractor.extract" href="#ieat.models.EmbeddingExtractor.extract">extract</a></code></li>
<li><code><a title="ieat.models.EmbeddingExtractor.extract_dir" href="#ieat.models.EmbeddingExtractor.extract_dir">extract_dir</a></code></li>
<li><code><a title="ieat.models.EmbeddingExtractor.load_model" href="#ieat.models.EmbeddingExtractor.load_model">load_model</a></code></li>
<li><code><a title="ieat.models.EmbeddingExtractor.process_samples" href="#ieat.models.EmbeddingExtractor.process_samples">process_samples</a></code></li>
<li><code><a title="ieat.models.EmbeddingExtractor.visualize" href="#ieat.models.EmbeddingExtractor.visualize">visualize</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ieat.models.ImageGPT2LMHeadModel"><code class="flex name class">
<span>class <span class="ident">ImageGPT2LMHeadModel</span></span>
<span>(</span><span>config)</span>
</code></dt>
<dd>
<div class="desc"><p>Extension of the HuggingFace <code>GPT2LMHeadModel</code> for iGPT.
From <a href="https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb">apeguero1</a>.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L536-L555" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ImageGPT2LMHeadModel(GPT2LMHeadModel):
        &#34;&#34;&#34;
        Extension of the HuggingFace `GPT2LMHeadModel` for iGPT.
        From [apeguero1](https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb).
        &#34;&#34;&#34;
        load_tf_weights = load_tf_weights_in_image_gpt2

        def __init__(self, config):
                super().__init__(config)
                self.lm_head = nn.Linear(config.n_embd, config.vocab_size - 1, bias=False)
                replace_ln(self, &#34;net&#34;, config)  # replace layer normalization
                for n in range(config.n_layer):
                        self.transformer.h[n].mlp.act = ImageGPT2LMHeadModel.gelu2  # replace activation

        def tie_weights(self):  # image-gpt doesn&#39;t tie output and input embeddings
                pass

        @staticmethod
        def gelu2(x):
                return x * torch.sigmoid(1.702 * x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.modeling_gpt2.GPT2LMHeadModel</li>
<li>transformers.modeling_gpt2.GPT2PreTrainedModel</li>
<li>transformers.modeling_utils.PreTrainedModel</li>
<li>torch.nn.modules.module.Module</li>
<li>transformers.modeling_utils.ModuleUtilsMixin</li>
<li>transformers.generation_utils.GenerationMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="ieat.models.ImageGPT2LMHeadModel.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ieat.models.ImageGPT2LMHeadModel.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="ieat.models.ImageGPT2LMHeadModel.gelu2"><code class="name flex">
<span>def <span class="ident">gelu2</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L553-L555" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@staticmethod
def gelu2(x):
        return x * torch.sigmoid(1.702 * x)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ieat.models.ImageGPT2LMHeadModel.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input_ids=None, past_key_values=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None, **kwargs) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>The :class:<code>~transformers.GPT2LMHeadModel</code> forward method, overrides the :func:<code>__call__</code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<h2 id="args">Args</h2>
<p>input_ids (:obj:<code>torch.LongTensor</code> of shape :obj:<code>(batch_size, input_ids_length)</code>):
:obj:<code>input_ids_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0].shape[-2]</code> (<code>sequence_length</code> of input past key value states).
Indices of input sequence tokens in the vocabulary.</p>
<pre><code>If &lt;code&gt;past\_key\_values&lt;/code&gt; is used, only &lt;code&gt;input\_ids&lt;/code&gt; that do not have their past calculated should be passed
as &lt;code&gt;input\_ids&lt;/code&gt;.

Indices can be obtained using :class:&lt;code&gt;transformers.GPT2Tokenizer&lt;/code&gt;.
See :func:&lt;code&gt;transformers.PreTrainedTokenizer.encode&lt;/code&gt; and
:func:&lt;code&gt;transformers.PreTrainedTokenizer.\_\_call\_\_&lt;/code&gt; for details.

`What are input IDs? &lt;../glossary.html#input-ids&gt;`__
</code></pre>
<p>past_key_values (:obj:<code>List[torch.FloatTensor]</code> of length :obj:<code>config.n_layers</code>):
Contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model
(see <code>past_key_values</code> output below). Can be used to speed up sequential decoding.
The <code>input_ids</code> which have their past given to this model should not be passed as <code>input_ids</code> as they have already been computed.
attention_mask (:obj:<code>torch.FloatTensor</code> of shape :obj:<code>(batch_size, sequence_length)</code>, <code>optional</code>, defaults to :obj:<code>None</code>):
Mask to avoid performing attention on padding token indices.
Mask values selected in <code>[0, 1]</code>:
<code>1</code> for tokens that are NOT MASKED, <code>0</code> for MASKED tokens.</p>
<pre><code>`What are attention masks? &lt;../glossary.html#attention-mask&gt;`__
</code></pre>
<p>token_type_ids (:obj:<code>torch.LongTensor</code> of shape :obj:<code>(batch_size, input_ids_length)</code>, <code>optional</code>, defaults to :obj:<code>None</code>):
<code>input_ids_length</code> = <code>sequence_length if </code>past<code>is None else 1
Segment token indices to indicate first and second portions of the inputs.
Indices are selected in &lt;code&gt;\[0, 1]&lt;/code&gt;: &lt;code&gt;0&lt;/code&gt; corresponds to a &lt;code&gt;sentence A&lt;/code&gt; token, &lt;code&gt;1&lt;/code&gt;
corresponds to a &lt;code&gt;sentence B&lt;/code&gt; token</code>What are token type IDs? &lt;../glossary.html#token-type-ids&gt;`_
position_ids (:obj:<code>torch.LongTensor</code> of shape :obj:<code>(batch_size, sequence_length)</code>, <code>optional</code>, defaults to :obj:<code>None</code>):
Indices of positions of each input sequence tokens in the position embeddings.
Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<pre><code>`What are position IDs? &lt;../glossary.html#position-ids&gt;`_
</code></pre>
<p>head_mask (:obj:<code>torch.FloatTensor</code> of shape :obj:<code>(num_heads,)</code> or :obj:<code>(num_layers, num_heads)</code>, <code>optional</code>, defaults to :obj:<code>None</code>):
Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code>[0, 1]</code>:
:obj:<code>1</code> indicates the head is <strong>not masked</strong>, :obj:<code>0</code> indicates the head is <strong>masked</strong>.
inputs_embeds (:obj:<code>torch.FloatTensor</code> of shape :obj:<code>(batch_size, sequence_length, hidden_size)</code>, <code>optional</code>, defaults to :obj:<code>None</code>):
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors
than the model's internal embedding lookup matrix.
If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see <code>past_key_values</code>).
use_cache (:obj:<code>bool</code>):
If <code>use_cache</code> is True, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see <code>past_key_values</code>). Defaults to <code>True</code>.
output_attentions (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>None</code>):
If set to <code>True</code>, the attentions tensors of all attention layers are returned. See <code>attentions</code> under returned tensors for more detail.
output_hidden_states (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>None</code>):
If set to <code>True</code>, the hidden states of all layers are returned. See <code>hidden_states</code> under returned tensors for more detail.
return_dict (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>None</code>):
If set to <code>True</code>, the model will return a :class:<code>~transformers.file_utils.ModelOutput</code> instead of a
plain tuple.</p>
<p>labels (:obj:<code>torch.LongTensor</code> of shape :obj:<code>(batch_size, sequence_length)</code>, <code>optional</code>, defaults to :obj:<code>None</code>):
Labels for language modeling.
Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set <code>labels = input_ids</code>
Indices are selected in <code>[-100, 0, ..., config.vocab_size]</code>
All labels set to <code>-100</code> are ignored (masked), the loss is only
computed for labels in <code>[0, &hellip;, config.vocab_size]</code></p>
<h2 id="returns">Returns</h2>
<p>:class:<code>~transformers.modeling_outputs.CausalLMOutputWithPast</code> or :obj:<code>tuple(torch.FloatTensor)</code>:
A :class:<code>~transformers.modeling_outputs.CausalLMOutputWithPast</code> (if <code>return_dict=True</code> is passed or when <code>config.return_dict=True</code>) or a
tuple of :obj:<code>torch.FloatTensor</code> comprising various elements depending on the configuration
(:class:<code>~transformers.GPT2Config</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (:obj:<code>torch.FloatTensor</code> of shape :obj:<code>(1,)</code>, <code>optional</code>, returned when :obj:<code>labels</code> is provided) &ndash; Language modeling loss (for next-token prediction).</li>
<li><strong>logits</strong> (:obj:<code>torch.FloatTensor</code> of shape :obj:<code>(batch_size, sequence_length, config.vocab_size)</code>) &ndash; Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</li>
<li><strong>past_key_values</strong> (:obj:<code>List[torch.FloatTensor]</code>, <code>optional</code>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &ndash; List of :obj:<code>torch.FloatTensor</code> of length :obj:<code>config.n_layers</code>,
with each tensor of shape
:obj:<code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</li>
</ul>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.
- <strong>hidden_states</strong> (:obj:<code>tuple(torch.FloatTensor)</code>, <code>optional</code>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &ndash; Tuple of :obj:<code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape :obj:<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- <strong>attentions</strong> (:obj:<code>tuple(torch.FloatTensor)</code>, <code>optional</code>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &ndash; Tuple of :obj:<code>torch.FloatTensor</code> (one for each layer) of shape
:obj:<code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.
Example::</p>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; from transformers import GPT2Tokenizer, GPT2LMHeadModel

&gt;&gt;&gt; tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
&gt;&gt;&gt; model = GPT2LMHeadModel.from_pretrained('gpt2', return_dict=True)

&gt;&gt;&gt; inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
&gt;&gt;&gt; outputs = model(**inputs, labels=inputs["input_ids"])
&gt;&gt;&gt; loss = outputs.loss
&gt;&gt;&gt; logits = outputs.logits
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/transformers/modeling_gpt2.py#L676-L756" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@add_start_docstrings_to_callable(GPT2_INPUTS_DOCSTRING)
@add_code_sample_docstrings(
    tokenizer_class=_TOKENIZER_FOR_DOC,
    checkpoint=&#34;gpt2&#34;,
    output_type=CausalLMOutputWithPast,
    config_class=_CONFIG_FOR_DOC,
)
def forward(
    self,
    input_ids=None,
    past_key_values=None,
    attention_mask=None,
    token_type_ids=None,
    position_ids=None,
    head_mask=None,
    inputs_embeds=None,
    encoder_hidden_states=None,
    encoder_attention_mask=None,
    labels=None,
    use_cache=None,
    output_attentions=None,
    output_hidden_states=None,
    return_dict=None,
    **kwargs,
):
    r&#34;&#34;&#34;
    labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`, defaults to :obj:`None`):
        Labels for language modeling.
        Note that the labels **are shifted** inside the model, i.e. you can set ``labels = input_ids``
        Indices are selected in ``[-100, 0, ..., config.vocab_size]``
        All labels set to ``-100`` are ignored (masked), the loss is only
        computed for labels in ``[0, ..., config.vocab_size]``
    &#34;&#34;&#34;
    if &#34;past&#34; in kwargs:
        warnings.warn(
            &#34;The `past` argument is deprecated and will be removed in a future version, use `past_key_values` instead.&#34;,
            FutureWarning,
        )
        past_key_values = kwargs.pop(&#34;past&#34;)
    assert kwargs == {}, f&#34;Unexpected keyword arguments: {list(kwargs.keys())}.&#34;
    return_dict = return_dict if return_dict is not None else self.config.use_return_dict

    transformer_outputs = self.transformer(
        input_ids,
        past_key_values=past_key_values,
        attention_mask=attention_mask,
        token_type_ids=token_type_ids,
        position_ids=position_ids,
        head_mask=head_mask,
        inputs_embeds=inputs_embeds,
        encoder_hidden_states=encoder_hidden_states,
        encoder_attention_mask=encoder_attention_mask,
        use_cache=use_cache,
        output_attentions=output_attentions,
        output_hidden_states=output_hidden_states,
        return_dict=return_dict,
    )
    hidden_states = transformer_outputs[0]

    lm_logits = self.lm_head(hidden_states)

    loss = None
    if labels is not None:
        # Shift so that tokens &lt; n predict n
        shift_logits = lm_logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        # Flatten the tokens
        loss_fct = CrossEntropyLoss()
        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

    if not return_dict:
        output = (lm_logits,) + transformer_outputs[1:]
        return ((loss,) + output) if loss is not None else output

    return CausalLMOutputWithPast(
        loss=loss,
        logits=lm_logits,
        past_key_values=transformer_outputs.past_key_values,
        hidden_states=transformer_outputs.hidden_states,
        attentions=transformer_outputs.attentions,
    )</code></pre>
</details>
</dd>
<dt id="ieat.models.ImageGPT2LMHeadModel.load_tf_weights"><code class="name flex">
<span>def <span class="ident">load_tf_weights</span></span>(<span>model, config, gpt2_checkpoint_path)</span>
</code></dt>
<dd>
<div class="desc"><p>Load tf checkpoints in a custom pytorch model.
From <a href="https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb">apeguero1</a>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L419-L523" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def load_tf_weights_in_image_gpt2(model, config, gpt2_checkpoint_path):
        &#34;&#34;&#34;
        Load tf checkpoints in a custom pytorch model.
        From [apeguero1](https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb).
        &#34;&#34;&#34;
        try:
                import re
                import tensorflow as tf
        except ImportError:
                logger.error(
                        &#34;Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see &#34;
                        &#34;https://www.tensorflow.org/install/ for installation instructions.&#34;
                )
                raise
        tf_path = os.path.abspath(gpt2_checkpoint_path)
        logger.debug(&#34;Converting TensorFlow checkpoint from {}&#34;.format(tf_path))
        # Load weights from TF model
        init_vars = tf.train.list_variables(tf_path)
        names = []
        arrays = []

        for name, shape in init_vars:
                logger.debug(&#34;Loading TF weight {} with shape {}&#34;.format(name, shape))
                array = tf.train.load_variable(tf_path, name)
                names.append(name)
                arrays.append(array.squeeze())

        for name, array in zip(names, arrays):
                name = name[6:]  # skip &#34;model/&#34;
                name = name.split(&#34;/&#34;)

                # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v
                # which are not required for using pretrained model
                if any(
                                n in [&#34;adam_v&#34;, &#34;adam_m&#34;, &#34;AdamWeightDecayOptimizer&#34;, &#34;AdamWeightDecayOptimizer_1&#34;, &#34;global_step&#34;]
                                for n in name
                ) or name[-1] in [&#39;_step&#39;]:
                        logger.debug(&#34;Skipping {}&#34;.format(&#34;/&#34;.join(name)))
                        continue

                pointer = model
                if name[-1] not in [&#34;wtet&#34;]:
                        pointer = getattr(pointer, &#34;transformer&#34;)

                for m_name in name:
                        if re.fullmatch(r&#34;[A-Za-z]+\d+&#34;, m_name):
                                scope_names = re.split(r&#34;(\d+)&#34;, m_name)
                        else:
                                scope_names = [m_name]

                        if scope_names[0] == &#34;w&#34; or scope_names[0] == &#34;g&#34;:
                                pointer = getattr(pointer, &#34;weight&#34;)
                        elif scope_names[0] == &#34;b&#34;:
                                pointer = getattr(pointer, &#34;bias&#34;)
                        elif scope_names[0] == &#34;wpe&#34; or scope_names[0] == &#34;wte&#34;:
                                pointer = getattr(pointer, scope_names[0])
                                pointer = getattr(pointer, &#34;weight&#34;)
                        elif scope_names[0] in [&#39;q_proj&#39;, &#39;k_proj&#39;, &#39;v_proj&#39;]:
                                pointer = getattr(pointer, &#39;c_attn&#39;)
                                pointer = getattr(pointer, &#39;weight&#39;)
                        elif len(name) == 3 and name[1] == &#34;attn&#34; and scope_names[0] == &#34;c_proj&#34;:
                                pointer = getattr(pointer, scope_names[0])
                                pointer = getattr(pointer, &#39;weight&#39;)
                        elif scope_names[0] == &#34;wtet&#34;:
                                pointer = getattr(pointer, &#34;lm_head&#34;)
                                pointer = getattr(pointer, &#39;weight&#39;)
                        elif scope_names[0] == &#34;sos&#34;:
                                pointer = getattr(pointer, &#34;wte&#34;)
                                pointer = getattr(pointer, &#39;weight&#39;)
                        else:
                                pointer = getattr(pointer, scope_names[0])
                        if len(scope_names) &gt;= 2:
                                num = int(scope_names[1])
                                pointer = pointer[num]

                if len(name) &gt; 1 and name[1] == &#34;attn&#34; or name[-1] == &#34;wtet&#34; or name[-1] == &#34;sos&#34; or name[-1] == &#34;wte&#34;:
                        pass  # array is used to initialize only part of the pointer so sizes won&#39;t match
                else:
                        try:
                                assert pointer.shape == array.shape
                        except AssertionError as e:
                                e.args += (pointer.shape, array.shape)
                                raise

                logger.debug(&#34;Initialize PyTorch weight {}&#34;.format(name))

                if name[-1] == &#34;q_proj&#34;:
                        pointer.data[:, :config.n_embd] = torch.from_numpy(array.reshape(config.n_embd, config.n_embd)).T
                elif name[-1] == &#34;k_proj&#34;:
                        pointer.data[:, config.n_embd:2 * config.n_embd] = torch.from_numpy(
                                array.reshape(config.n_embd, config.n_embd)).T
                elif name[-1] == &#34;v_proj&#34;:
                        pointer.data[:, 2 * config.n_embd:] = torch.from_numpy(array.reshape(config.n_embd, config.n_embd)).T
                elif len(name) == 3 and name[1] == &#34;attn&#34; and name[2] == &#34;c_proj&#34;:
                        pointer.data = torch.from_numpy(array.reshape(config.n_embd, config.n_embd))
                elif name[-1] == &#34;wtet&#34;:
                        pointer.data = torch.from_numpy(array)
                elif name[-1] == &#34;wte&#34;:
                        pointer.data[:config.vocab_size - 1, :] = torch.from_numpy(array)
                elif name[-1] == &#34;sos&#34;:
                        pointer.data[-1] = torch.from_numpy(array)
                else:
                        pointer.data = torch.from_numpy(array)

        return model</code></pre>
</details>
</dd>
<dt id="ieat.models.ImageGPT2LMHeadModel.tie_weights"><code class="name flex">
<span>def <span class="ident">tie_weights</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Tie the weights between the input embeddings and the output embeddings.</p>
<p>If the :obj:<code>torchscript</code> flag is set in the configuration, can't handle parameter sharing so we are cloning
the weights instead.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L550-L551" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def tie_weights(self):  # image-gpt doesn&#39;t tie output and input embeddings
        pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ieat.models.LogitExtractor"><code class="flex name class">
<span>class <span class="ident">LogitExtractor</span></span>
<span>(</span><span>model_name, model_size, models_dir, color_clusters_dir, n_px, **parent_params)</span>
</code></dt>
<dd>
<div class="desc"><p>Extractor for iGPT logit (projection head) layer.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_name</code></strong> :&ensp;<code>str</code></dt>
<dd>A name for this model, used for caching.</dd>
<dt><strong><code>model_size</code></strong> :&ensp;<code>str</code></dt>
<dd>The size of iGPT used - "s" for small, "m" for medium, or "l" for large. The exact parameters are stored in
<code><a title="ieat.models.GPTExtractor.MODELS" href="#ieat.models.GPTExtractor.MODELS">GPTExtractor.MODELS</a></code>.</dd>
<dt><strong><code>models_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to directory with downloaded model. Make sure the params match the downloaded model.</dd>
<dt><strong><code>color_clusters_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to directory with the downloaded color clusters.</dd>
<dt><strong><code>n_px</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of pixels used. All publicly available versions of iGPT are 32x32.</dd>
<dt><strong><code>parent_params</code></strong></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L356-L364" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class LogitExtractor(GPTExtractor):
        &#34;&#34;&#34;Extractor for iGPT logit (projection head) layer.&#34;&#34;&#34;
        def _extract_context(self, samples, gpu, **extract_kwargs) -&gt; np.ndarray:
                output = self.model_output(samples, gpu)
                # just use the logit layer
                # extract the rep of the last input, as in sent-bias
                enc_last = output.logits[:, -1, :]

                return enc_last.numpy() if not gpu else enc_last.cpu().numpy()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ieat.models.GPTExtractor" href="#ieat.models.GPTExtractor">GPTExtractor</a></li>
<li><a title="ieat.models.EmbeddingExtractor" href="#ieat.models.EmbeddingExtractor">EmbeddingExtractor</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ieat.models.GPTExtractor" href="#ieat.models.GPTExtractor">GPTExtractor</a></b></code>:
<ul class="hlist">
<li><code><a title="ieat.models.GPTExtractor.extract" href="#ieat.models.EmbeddingExtractor.extract">extract</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.extract_dir" href="#ieat.models.EmbeddingExtractor.extract_dir">extract_dir</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.load_model" href="#ieat.models.EmbeddingExtractor.load_model">load_model</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.model_output" href="#ieat.models.GPTExtractor.model_output">model_output</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.process_samples" href="#ieat.models.EmbeddingExtractor.process_samples">process_samples</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.visualize" href="#ieat.models.EmbeddingExtractor.visualize">visualize</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ieat.models.OpenAIExtractor"><code class="flex name class">
<span>class <span class="ident">OpenAIExtractor</span></span>
<span>(</span><span>model_name, model_size, models_dir, color_clusters_dir, n_px, **parent_params)</span>
</code></dt>
<dd>
<div class="desc"><p>Pooled extraction method, used by the iGPT authors for linear evaluation.
1. find $n^l = layer_norm(h^l)$
2. average pool across the sequence dimension:
$$ f^l = \langle n^l_i
angle_i $$</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_name</code></strong> :&ensp;<code>str</code></dt>
<dd>A name for this model, used for caching.</dd>
<dt><strong><code>model_size</code></strong> :&ensp;<code>str</code></dt>
<dd>The size of iGPT used - "s" for small, "m" for medium, or "l" for large. The exact parameters are stored in
<code><a title="ieat.models.GPTExtractor.MODELS" href="#ieat.models.GPTExtractor.MODELS">GPTExtractor.MODELS</a></code>.</dd>
<dt><strong><code>models_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to directory with downloaded model. Make sure the params match the downloaded model.</dd>
<dt><strong><code>color_clusters_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to directory with the downloaded color clusters.</dd>
<dt><strong><code>n_px</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of pixels used. All publicly available versions of iGPT are 32x32.</dd>
<dt><strong><code>parent_params</code></strong></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L383-L400" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class OpenAIExtractor(GPTExtractor):
        &#34;&#34;&#34;
        Pooled extraction method, used by the iGPT authors for linear evaluation.
        1. find $n^l = layer\_norm(h^l)$
        2. average pool across the sequence dimension:
        $$ f^l = \langle n^l_i \rangle_i $$
        &#34;&#34;&#34;
        def _extract_context(self, samples, gpu, **extract_kwargs) -&gt; np.ndarray:
                l = extract_kwargs.get(&#34;l&#34;, 20)

                output = self.model_output(samples, gpu)

                # extract the rep of the lth input
                h_l = output.hidden_states[l]
                norm = self.model.transformer.h[l+1].ln_1(h_l)
                enc = tf.reduce_mean(norm, axis=1)

                return enc.numpy() if not gpu else enc.cpu().numpy()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ieat.models.GPTExtractor" href="#ieat.models.GPTExtractor">GPTExtractor</a></li>
<li><a title="ieat.models.EmbeddingExtractor" href="#ieat.models.EmbeddingExtractor">EmbeddingExtractor</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ieat.models.GPTExtractor" href="#ieat.models.GPTExtractor">GPTExtractor</a></b></code>:
<ul class="hlist">
<li><code><a title="ieat.models.GPTExtractor.extract" href="#ieat.models.EmbeddingExtractor.extract">extract</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.extract_dir" href="#ieat.models.EmbeddingExtractor.extract_dir">extract_dir</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.load_model" href="#ieat.models.EmbeddingExtractor.load_model">load_model</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.model_output" href="#ieat.models.GPTExtractor.model_output">model_output</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.process_samples" href="#ieat.models.EmbeddingExtractor.process_samples">process_samples</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.visualize" href="#ieat.models.EmbeddingExtractor.visualize">visualize</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ieat.models.SENTExtractor"><code class="flex name class">
<span>class <span class="ident">SENTExtractor</span></span>
<span>(</span><span>model_name, model_size, models_dir, color_clusters_dir, n_px, **parent_params)</span>
</code></dt>
<dd>
<div class="desc"><p>Extractor for last position of the last layer output.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_name</code></strong> :&ensp;<code>str</code></dt>
<dd>A name for this model, used for caching.</dd>
<dt><strong><code>model_size</code></strong> :&ensp;<code>str</code></dt>
<dd>The size of iGPT used - "s" for small, "m" for medium, or "l" for large. The exact parameters are stored in
<code><a title="ieat.models.GPTExtractor.MODELS" href="#ieat.models.GPTExtractor.MODELS">GPTExtractor.MODELS</a></code>.</dd>
<dt><strong><code>models_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to directory with downloaded model. Make sure the params match the downloaded model.</dd>
<dt><strong><code>color_clusters_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to directory with the downloaded color clusters.</dd>
<dt><strong><code>n_px</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of pixels used. All publicly available versions of iGPT are 32x32.</dd>
<dt><strong><code>parent_params</code></strong></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L367-L380" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class SENTExtractor(GPTExtractor):
        &#34;&#34;&#34;Extractor for last position of the last layer output.&#34;&#34;&#34;
        def _extract_context(self, samples, gpu, **extract_kwargs)  -&gt; np.ndarray:
                &#34;&#34;&#34;
                SENT uses the last hidden layer output.

                For details, see https://github.com/tanyichern/social-biases-contextualized/blob/master/gpt2.py.
                &#34;&#34;&#34;
                # initialize with SOS token
                output = self.model_output(samples, gpu)

                enc_last = output.hidden_states[-1][:, -1, :] # extract the rep of the last input

                return enc_last.numpy() if not gpu else enc_last.cpu().numpy()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ieat.models.GPTExtractor" href="#ieat.models.GPTExtractor">GPTExtractor</a></li>
<li><a title="ieat.models.EmbeddingExtractor" href="#ieat.models.EmbeddingExtractor">EmbeddingExtractor</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ieat.models.GPTExtractor" href="#ieat.models.GPTExtractor">GPTExtractor</a></b></code>:
<ul class="hlist">
<li><code><a title="ieat.models.GPTExtractor.extract" href="#ieat.models.EmbeddingExtractor.extract">extract</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.extract_dir" href="#ieat.models.EmbeddingExtractor.extract_dir">extract_dir</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.load_model" href="#ieat.models.EmbeddingExtractor.load_model">load_model</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.model_output" href="#ieat.models.GPTExtractor.model_output">model_output</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.process_samples" href="#ieat.models.EmbeddingExtractor.process_samples">process_samples</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.visualize" href="#ieat.models.EmbeddingExtractor.visualize">visualize</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ieat.models.SimCLRExtractor"><code class="flex name class">
<span>class <span class="ident">SimCLRExtractor</span></span>
<span>(</span><span>model_name: str, depth: int, width: int, sk: int, **parent_params)</span>
</code></dt>
<dd>
<div class="desc"><p>Extractor using the <a href="https://github.com/google-research/simclr">SimCLR model</a>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_name</code></strong> :&ensp;<code>str</code></dt>
<dd>A name for this model, used for caching.</dd>
<dt><strong><code>depth</code></strong> :&ensp;<code>int</code></dt>
<dd>Depth of the ResNet used.</dd>
<dt><strong><code>width</code></strong> :&ensp;<code>int</code></dt>
<dd>Width of the resnet used.</dd>
<dt><strong><code>sk</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use selective kernels.</dd>
<dt><strong><code>parent_params</code></strong></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L197-L246" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class SimCLRExtractor(EmbeddingExtractor):
        &#34;&#34;&#34;Extractor using the [SimCLR model](https://github.com/google-research/simclr).&#34;&#34;&#34;
        n_px = 224

        def __init__(self, model_name: str, depth: int, width: int, sk: int, **parent_params):
                &#34;&#34;&#34;
                Parameters
                ----------
                model_name : str
                        A name for this model, used for caching.
                depth : int
                        Depth of the ResNet used.
                width : int
                        Width of the resnet used.
                sk : bool
                        Whether to use selective kernels.
                parent_params
                &#34;&#34;&#34;
                super().__init__(model_name, **parent_params)
                tf.compat.v1.disable_eager_execution()
                self.depth = depth
                self.width = width
                self.sk = sk
                self.sess = None
                self.images = None

        def load_model(self):
                hub_path = f&#34;gs://simclr-checkpoints/simclrv2/pretrained/r{self.depth}_{self.width}x_sk{self.sk}/hub&#34;
                module = hub.Module(hub_path, trainable=False)
                self.images = tf.compat.v1.placeholder(tf.float32)
                self.model = module(inputs=self.images, signature=&#34;default&#34;, as_dict=True)
                self.sess = tf.compat.v1.Session()
                self.sess.run(tf.compat.v1.global_variables_initializer())

        def process_samples(self, image_paths: list, visualize=False):
                images = np.array([image/255 for image in resize(SimCLRExtractor.n_px, image_paths)])

                if visualize:
                        self.visualize(images, image_paths)

                return images

        def _extract_context(self, samples, gpu, **extract_kwargs) -&gt; np.ndarray:
                output = self.sess.run(self.model, {self.images: samples})
                # &#39;default&#39; is the representation output of the base ResNet network
                encs = output[&#39;default&#39;]
                return encs

        def _make_param_path(self):
                return f&#34;{self.depth}_{self.width}x_sk{self.sk}&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ieat.models.EmbeddingExtractor" href="#ieat.models.EmbeddingExtractor">EmbeddingExtractor</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="ieat.models.SimCLRExtractor.n_px"><code class="name">var <span class="ident">n_px</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ieat.models.EmbeddingExtractor" href="#ieat.models.EmbeddingExtractor">EmbeddingExtractor</a></b></code>:
<ul class="hlist">
<li><code><a title="ieat.models.EmbeddingExtractor.extract" href="#ieat.models.EmbeddingExtractor.extract">extract</a></code></li>
<li><code><a title="ieat.models.EmbeddingExtractor.extract_dir" href="#ieat.models.EmbeddingExtractor.extract_dir">extract_dir</a></code></li>
<li><code><a title="ieat.models.EmbeddingExtractor.load_model" href="#ieat.models.EmbeddingExtractor.load_model">load_model</a></code></li>
<li><code><a title="ieat.models.EmbeddingExtractor.process_samples" href="#ieat.models.EmbeddingExtractor.process_samples">process_samples</a></code></li>
<li><code><a title="ieat.models.EmbeddingExtractor.visualize" href="#ieat.models.EmbeddingExtractor.visualize">visualize</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ieat.models.ln_mod"><code class="flex name class">
<span>class <span class="ident">ln_mod</span></span>
<span>(</span><span>nx, eps=1e-05)</span>
</code></dt>
<dd>
<div class="desc"><p>Torch module for the iGPT modified linear head.
From <a href="https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb">apeguero1</a>.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L403-L416" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ln_mod(nn.Module):
        &#34;&#34;&#34;
        Torch module for the iGPT modified linear head.
        From [apeguero1](https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb).
        &#34;&#34;&#34;
        def __init__(self, nx, eps=1e-5):
                super().__init__()
                self.eps = eps
                self.weight = Parameter(torch.Tensor(nx))

        def forward(self, x):  # input is not mean centered
                return x \
                        / torch.sqrt(torch.std(x, axis=-1, unbiased=False, keepdim=True) ** 2 + self.eps) \
                        * self.weight.data[..., :]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="ieat.models.ln_mod.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ieat.models.ln_mod.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ieat.models.ln_mod.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ryansteed/ieat/blob/7d41e8ef5b02b9e30cb53424bdafbb3b79b3abe0/ieat/models.py#L413-L416" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def forward(self, x):  # input is not mean centered
        return x \
                / torch.sqrt(torch.std(x, axis=-1, unbiased=False, keepdim=True) ** 2 + self.eps) \
                * self.weight.data[..., :]</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ieat" href="index.html">ieat</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ieat.models.load_tf_weights_in_image_gpt2" href="#ieat.models.load_tf_weights_in_image_gpt2">load_tf_weights_in_image_gpt2</a></code></li>
<li><code><a title="ieat.models.replace_ln" href="#ieat.models.replace_ln">replace_ln</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ieat.models.EmbeddingExtractor" href="#ieat.models.EmbeddingExtractor">EmbeddingExtractor</a></code></h4>
<ul class="">
<li><code><a title="ieat.models.EmbeddingExtractor.extract" href="#ieat.models.EmbeddingExtractor.extract">extract</a></code></li>
<li><code><a title="ieat.models.EmbeddingExtractor.extract_dir" href="#ieat.models.EmbeddingExtractor.extract_dir">extract_dir</a></code></li>
<li><code><a title="ieat.models.EmbeddingExtractor.load_model" href="#ieat.models.EmbeddingExtractor.load_model">load_model</a></code></li>
<li><code><a title="ieat.models.EmbeddingExtractor.process_samples" href="#ieat.models.EmbeddingExtractor.process_samples">process_samples</a></code></li>
<li><code><a title="ieat.models.EmbeddingExtractor.visualize" href="#ieat.models.EmbeddingExtractor.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ieat.models.GPTExtractor" href="#ieat.models.GPTExtractor">GPTExtractor</a></code></h4>
<ul class="">
<li><code><a title="ieat.models.GPTExtractor.MODELS" href="#ieat.models.GPTExtractor.MODELS">MODELS</a></code></li>
<li><code><a title="ieat.models.GPTExtractor.model_output" href="#ieat.models.GPTExtractor.model_output">model_output</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ieat.models.ImageGPT2LMHeadModel" href="#ieat.models.ImageGPT2LMHeadModel">ImageGPT2LMHeadModel</a></code></h4>
<ul class="two-column">
<li><code><a title="ieat.models.ImageGPT2LMHeadModel.dump_patches" href="#ieat.models.ImageGPT2LMHeadModel.dump_patches">dump_patches</a></code></li>
<li><code><a title="ieat.models.ImageGPT2LMHeadModel.forward" href="#ieat.models.ImageGPT2LMHeadModel.forward">forward</a></code></li>
<li><code><a title="ieat.models.ImageGPT2LMHeadModel.gelu2" href="#ieat.models.ImageGPT2LMHeadModel.gelu2">gelu2</a></code></li>
<li><code><a title="ieat.models.ImageGPT2LMHeadModel.load_tf_weights" href="#ieat.models.ImageGPT2LMHeadModel.load_tf_weights">load_tf_weights</a></code></li>
<li><code><a title="ieat.models.ImageGPT2LMHeadModel.tie_weights" href="#ieat.models.ImageGPT2LMHeadModel.tie_weights">tie_weights</a></code></li>
<li><code><a title="ieat.models.ImageGPT2LMHeadModel.training" href="#ieat.models.ImageGPT2LMHeadModel.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ieat.models.LogitExtractor" href="#ieat.models.LogitExtractor">LogitExtractor</a></code></h4>
</li>
<li>
<h4><code><a title="ieat.models.OpenAIExtractor" href="#ieat.models.OpenAIExtractor">OpenAIExtractor</a></code></h4>
</li>
<li>
<h4><code><a title="ieat.models.SENTExtractor" href="#ieat.models.SENTExtractor">SENTExtractor</a></code></h4>
</li>
<li>
<h4><code><a title="ieat.models.SimCLRExtractor" href="#ieat.models.SimCLRExtractor">SimCLRExtractor</a></code></h4>
<ul class="">
<li><code><a title="ieat.models.SimCLRExtractor.n_px" href="#ieat.models.SimCLRExtractor.n_px">n_px</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ieat.models.ln_mod" href="#ieat.models.ln_mod">ln_mod</a></code></h4>
<ul class="">
<li><code><a title="ieat.models.ln_mod.dump_patches" href="#ieat.models.ln_mod.dump_patches">dump_patches</a></code></li>
<li><code><a title="ieat.models.ln_mod.forward" href="#ieat.models.ln_mod.forward">forward</a></code></li>
<li><code><a title="ieat.models.ln_mod.training" href="#ieat.models.ln_mod.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>