{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image-GPT-Bias-HuggingFace.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9tObiU-qVgv",
        "colab_type": "text"
      },
      "source": [
        "#Image GPT Bias Analysis w/ Hugging Face\n",
        "Ryan Steed\n",
        "\n",
        "Adapted from https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoWYiUzT_lLs",
        "colab_type": "text"
      },
      "source": [
        "## Download Image GPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzJbEWC6Vogc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "eab0c061-c966-40c2-d376-a02cb6e9c025"
      },
      "source": [
        "!nvidia-smi #OpenAI says you need 16GB GPU for the large model, but it may work if you lower n_sub_batch on the others."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Aug 27 15:10:50 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkPOYJsCTaUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bbfedd17-d26e-4ecd-f922-f74a5a0f2fbb"
      },
      "source": [
        "!git clone https://github.com/openai/image-gpt.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'image-gpt'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Total 41 (delta 0), reused 0 (delta 0), pack-reused 41\u001b[K\n",
            "Unpacking objects: 100% (41/41), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0DcaUYv8LYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_sizes = [\"s\", \"m\", \"l\"] #small medium large, xl not available\n",
        "model_size = \"m\"\n",
        "models_dir = \"/content/models\"\n",
        "color_clusters_dir = \"/content/clusters\"\n",
        "bs = 8\n",
        "n_px = 32"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylcjIJcwXsFw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "2969d06e-bc77-4fe2-f930-5ff2ae41cfa9"
      },
      "source": [
        "!python image-gpt/download.py --model {model_size} --ckpt 1000000 --clusters --download_dir {models_dir}/{model_size}\n",
        "!python image-gpt/download.py --clusters --download_dir {color_clusters_dir}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input args:\n",
            " {\n",
            "    \"download_dir\":\"/content/models/m\",\n",
            "    \"model\":\"m\",\n",
            "    \"ckpt\":\"1000000\",\n",
            "    \"clusters\":true,\n",
            "    \"dataset\":null\n",
            "}\n",
            "Fetching model.ckpt-1000000.data-00000-of-00032: 1.00kit [00:00, 673kit/s]      \n",
            "Fetching model.ckpt-1000000.data-00001-of-00032: 168Mit [00:05, 31.6Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00002-of-00032: 182Mit [00:04, 41.4Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00003-of-00032: 185Mit [00:04, 42.4Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00004-of-00032: 180Mit [00:04, 43.5Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00005-of-00032: 172Mit [00:03, 43.4Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00006-of-00032: 172Mit [00:04, 36.3Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00007-of-00032: 172Mit [00:03, 47.4Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00008-of-00032: 185Mit [00:06, 27.8Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00009-of-00032: 182Mit [00:05, 32.7Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00010-of-00032: 172Mit [00:05, 31.0Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00011-of-00032: 185Mit [00:03, 53.9Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00012-of-00032: 172Mit [00:06, 27.1Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00013-of-00032: 176Mit [00:04, 40.5Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00014-of-00032: 180Mit [00:03, 59.9Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00015-of-00032: 172Mit [00:03, 49.3Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00016-of-00032: 176Mit [00:04, 39.0Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00017-of-00032: 172Mit [00:04, 36.9Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00018-of-00032: 182Mit [00:04, 36.9Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00019-of-00032: 172Mit [00:05, 34.0Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00020-of-00032: 185Mit [00:05, 34.5Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00021-of-00032: 172Mit [00:05, 30.8Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00022-of-00032: 180Mit [00:05, 32.0Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00023-of-00032: 172Mit [00:07, 24.5Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00024-of-00032: 176Mit [00:04, 39.9Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00025-of-00032: 178Mit [00:03, 46.9Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00026-of-00032: 170Mit [00:03, 46.7Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00027-of-00032: 176Mit [00:02, 59.4Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00028-of-00032: 180Mit [00:04, 38.6Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00029-of-00032: 172Mit [00:04, 41.1Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00030-of-00032: 172Mit [00:05, 34.3Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00031-of-00032: 170Mit [00:05, 32.6Mit/s]      \n",
            "Fetching model.ckpt-1000000.index: 13.0kit [00:00, 7.01Mit/s]                   \n",
            "Fetching model.ckpt-1000000.meta: 20.5Mit [00:00, 39.2Mit/s]                    \n",
            "Fetching kmeans_centers.npy: 7.00kit [00:00, 4.65Mit/s]                         \n",
            "input args:\n",
            " {\n",
            "    \"download_dir\":\"/content/clusters\",\n",
            "    \"model\":null,\n",
            "    \"ckpt\":null,\n",
            "    \"clusters\":true,\n",
            "    \"dataset\":null\n",
            "}\n",
            "Fetching kmeans_centers.npy: 7.00kit [00:00, 6.52Mit/s]                         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjVa2W5f-6BB",
        "colab_type": "text"
      },
      "source": [
        "## Subclass GPT2LMHeadModel\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoImkBA2d5jT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "4e28112b-78c2-4697-e6f9-c5e34876fab0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 19.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 26.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=ce2433de68199bfa53dae26d9d46988b2ac237b3b8bb5064e437abdbb027767a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soYP5NE_KPZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import transformers\n",
        "from transformers.modeling_gpt2 import GPT2Model,GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def load_tf_weights_in_image_gpt2(model, config, gpt2_checkpoint_path):\n",
        "    \"\"\" Load tf checkpoints in a pytorch model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import re\n",
        "        import tensorflow as tf\n",
        "    except ImportError:\n",
        "        logger.error(\n",
        "            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n",
        "            \"https://www.tensorflow.org/install/ for installation instructions.\"\n",
        "        )\n",
        "        raise\n",
        "    tf_path = os.path.abspath(gpt2_checkpoint_path)\n",
        "    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n",
        "    # Load weights from TF model\n",
        "    init_vars = tf.train.list_variables(tf_path)\n",
        "    names = []\n",
        "    arrays = []\n",
        "\n",
        "    for name, shape in init_vars:\n",
        "        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
        "        array = tf.train.load_variable(tf_path, name)\n",
        "        names.append(name)\n",
        "        arrays.append(array.squeeze())\n",
        "\n",
        "    for name, array in zip(names, arrays):\n",
        "        name = name[6:]  # skip \"model/\"\n",
        "        name = name.split(\"/\")\n",
        "\n",
        "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
        "        # which are not required for using pretrained model\n",
        "        if any(\n",
        "            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n",
        "            for n in name\n",
        "        ) or name[-1] in ['_step']:\n",
        "            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
        "            continue\n",
        "        \n",
        "        pointer = model\n",
        "        if name[-1] not in [\"wtet\"]:\n",
        "          pointer = getattr(pointer, \"transformer\")\n",
        "        \n",
        "        for m_name in name:\n",
        "            if re.fullmatch(r\"[A-Za-z]+\\d+\", m_name):\n",
        "                scope_names = re.split(r\"(\\d+)\", m_name)\n",
        "            else:\n",
        "                scope_names = [m_name]\n",
        "\n",
        "            if scope_names[0] == \"w\" or scope_names[0] == \"g\":\n",
        "                pointer = getattr(pointer, \"weight\")\n",
        "            elif scope_names[0] == \"b\":\n",
        "                pointer = getattr(pointer, \"bias\")\n",
        "            elif scope_names[0] == \"wpe\" or scope_names[0] == \"wte\":\n",
        "                pointer = getattr(pointer, scope_names[0])\n",
        "                pointer = getattr(pointer, \"weight\")\n",
        "            elif scope_names[0] in ['q_proj','k_proj','v_proj']:\n",
        "                pointer = getattr(pointer, 'c_attn')\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif len(name) ==3 and name[1]==\"attn\" and scope_names[0]==\"c_proj\":\n",
        "                pointer = getattr(pointer, scope_names[0])\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif scope_names[0]==\"wtet\":\n",
        "                pointer = getattr(pointer, \"lm_head\")\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif scope_names[0]==\"sos\":\n",
        "                pointer = getattr(pointer,\"wte\")\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            else:\n",
        "                pointer = getattr(pointer, scope_names[0])\n",
        "            if len(scope_names) >= 2:\n",
        "                num = int(scope_names[1])\n",
        "                pointer = pointer[num]\n",
        "\n",
        "        if len(name) > 1 and name[1]==\"attn\" or name[-1]==\"wtet\" or name[-1]==\"sos\" or name[-1]==\"wte\":\n",
        "           pass #array is used to initialize only part of the pointer so sizes won't match\n",
        "        else:\n",
        "          try:\n",
        "              assert pointer.shape == array.shape\n",
        "          except AssertionError as e:\n",
        "              e.args += (pointer.shape, array.shape)\n",
        "              raise\n",
        "          \n",
        "        logger.info(\"Initialize PyTorch weight {}\".format(name))\n",
        "\n",
        "        if name[-1]==\"q_proj\":\n",
        "          pointer.data[:,:config.n_embd] = torch.from_numpy(array.reshape(config.n_embd,config.n_embd) ).T\n",
        "        elif name[-1]==\"k_proj\":\n",
        "          pointer.data[:,config.n_embd:2*config.n_embd] = torch.from_numpy(array.reshape(config.n_embd,config.n_embd) ).T\n",
        "        elif name[-1]==\"v_proj\":\n",
        "          pointer.data[:,2*config.n_embd:] = torch.from_numpy(array.reshape(config.n_embd,config.n_embd) ).T\n",
        "        elif (len(name) ==3 and name[1]==\"attn\" and name[2]==\"c_proj\" ):\n",
        "          pointer.data = torch.from_numpy(array.reshape(config.n_embd,config.n_embd) )\n",
        "        elif name[-1]==\"wtet\":\n",
        "          pointer.data = torch.from_numpy(array)\n",
        "        elif name[-1]==\"wte\":\n",
        "          pointer.data[:config.vocab_size-1,:] = torch.from_numpy(array)\n",
        "        elif name[-1]==\"sos\":\n",
        "          pointer.data[-1] = torch.from_numpy(array)\n",
        "        else:\n",
        "          pointer.data = torch.from_numpy(array)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "class ln_mod(nn.Module):\n",
        "    def __init__(self, nx,eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = Parameter(torch.Tensor(nx))\n",
        "    def forward(self,x):#input is not mean centered\n",
        "        return x / torch.sqrt( torch.std(x,axis=-1,unbiased=False,keepdim=True)**2 + self.eps ) * self.weight.data[...,:] \n",
        "\n",
        "def replace_ln(m, name,config):\n",
        "  for attr_str in dir(m):\n",
        "      target_attr = getattr(m, attr_str)\n",
        "      if type(target_attr) == torch.nn.LayerNorm:\n",
        "          #print('replaced: ', name, attr_str)\n",
        "          setattr(m, attr_str, ln_mod(config.n_embd,config.layer_norm_epsilon))\n",
        "\n",
        "  for n, ch in m.named_children():\n",
        "      replace_ln(ch, n,config)        \n",
        "\n",
        "def gelu2(x):\n",
        "    return x * torch.sigmoid(1.702 * x)\n",
        "\n",
        "class ImageGPT2LMHeadModel(GPT2LMHeadModel):\n",
        "  load_tf_weights = load_tf_weights_in_image_gpt2\n",
        "  \n",
        "  def __init__(self, config):\n",
        "      super().__init__(config)\n",
        "      self.lm_head = nn.Linear(config.n_embd, config.vocab_size - 1, bias=False)\n",
        "      replace_ln(self,\"net\",config) #replace layer normalization\n",
        "      for n in range(config.n_layer):\n",
        "        self.transformer.h[n].mlp.act = gelu2 #replace activation \n",
        "\n",
        "  def tie_weights(self): #image-gpt doesn't tie output and input embeddings\n",
        "    pass "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxJvFFK8gqJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "color_clusters_file = \"%s/kmeans_centers.npy\"%(color_clusters_dir)\n",
        "clusters = np.load(color_clusters_file) #get color clusters"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q00XounhcEIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODELS={\"l\":(1536,16,48),\"m\":(1024,8,36),\"s\":(512,8,24) } \n",
        "n_embd,n_head,n_layer=MODELS[model_size] #set model hyperparameters\n",
        "vocab_size = len(clusters) + 1 #add one for start of sentence token\n",
        "config = transformers.GPT2Config(vocab_size=vocab_size,n_ctx=n_px*n_px,n_positions=n_px*n_px,n_embd=n_embd,n_layer=n_layer,n_head=n_head)\n",
        "model_path = \"%s/%s/model.ckpt-1000000.index\"%(models_dir,model_size)\n",
        "\n",
        "model = ImageGPT2LMHeadModel.from_pretrained(model_path,from_tf=True,config=config).cuda()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-kUzrTceiwk",
        "colab_type": "text"
      },
      "source": [
        "## Embedding Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gckv13lOekIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}