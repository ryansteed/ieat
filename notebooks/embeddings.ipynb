{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image-GPT-Bias-HuggingFace.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9tObiU-qVgv",
        "colab_type": "text"
      },
      "source": [
        "#Image GPT Bias Analysis w/ Hugging Face\n",
        "Ryan Steed\n",
        "\n",
        "Adapted from https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoWYiUzT_lLs",
        "colab_type": "text"
      },
      "source": [
        "## Download Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzJbEWC6Vogc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "eab0c061-c966-40c2-d376-a02cb6e9c025"
      },
      "source": [
        "!nvidia-smi #OpenAI says you need 16GB GPU for the large model, but it may work if you lower n_sub_batch on the others."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Aug 27 15:10:50 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkPOYJsCTaUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bbfedd17-d26e-4ecd-f922-f74a5a0f2fbb"
      },
      "source": [
        "!git clone https://github.com/openai/image-gpt.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'image-gpt'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Total 41 (delta 0), reused 0 (delta 0), pack-reused 41\u001b[K\n",
            "Unpacking objects: 100% (41/41), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0DcaUYv8LYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_sizes = [\"s\", \"m\", \"l\"] #small medium large, xl not available\n",
        "model_size = \"m\"\n",
        "models_dir = \"/content/models\"\n",
        "color_clusters_dir = \"/content/clusters\"\n",
        "bs = 8\n",
        "n_px = 32"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylcjIJcwXsFw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "2969d06e-bc77-4fe2-f930-5ff2ae41cfa9"
      },
      "source": [
        "!python image-gpt/download.py --model {model_size} --ckpt 1000000 --clusters --download_dir {models_dir}/{model_size}\n",
        "!python image-gpt/download.py --clusters --download_dir {color_clusters_dir}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input args:\n",
            " {\n",
            "    \"download_dir\":\"/content/models/m\",\n",
            "    \"model\":\"m\",\n",
            "    \"ckpt\":\"1000000\",\n",
            "    \"clusters\":true,\n",
            "    \"dataset\":null\n",
            "}\n",
            "Fetching model.ckpt-1000000.data-00000-of-00032: 1.00kit [00:00, 673kit/s]      \n",
            "Fetching model.ckpt-1000000.data-00001-of-00032: 168Mit [00:05, 31.6Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00002-of-00032: 182Mit [00:04, 41.4Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00003-of-00032: 185Mit [00:04, 42.4Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00004-of-00032: 180Mit [00:04, 43.5Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00005-of-00032: 172Mit [00:03, 43.4Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00006-of-00032: 172Mit [00:04, 36.3Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00007-of-00032: 172Mit [00:03, 47.4Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00008-of-00032: 185Mit [00:06, 27.8Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00009-of-00032: 182Mit [00:05, 32.7Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00010-of-00032: 172Mit [00:05, 31.0Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00011-of-00032: 185Mit [00:03, 53.9Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00012-of-00032: 172Mit [00:06, 27.1Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00013-of-00032: 176Mit [00:04, 40.5Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00014-of-00032: 180Mit [00:03, 59.9Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00015-of-00032: 172Mit [00:03, 49.3Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00016-of-00032: 176Mit [00:04, 39.0Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00017-of-00032: 172Mit [00:04, 36.9Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00018-of-00032: 182Mit [00:04, 36.9Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00019-of-00032: 172Mit [00:05, 34.0Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00020-of-00032: 185Mit [00:05, 34.5Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00021-of-00032: 172Mit [00:05, 30.8Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00022-of-00032: 180Mit [00:05, 32.0Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00023-of-00032: 172Mit [00:07, 24.5Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00024-of-00032: 176Mit [00:04, 39.9Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00025-of-00032: 178Mit [00:03, 46.9Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00026-of-00032: 170Mit [00:03, 46.7Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00027-of-00032: 176Mit [00:02, 59.4Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00028-of-00032: 180Mit [00:04, 38.6Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00029-of-00032: 172Mit [00:04, 41.1Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00030-of-00032: 172Mit [00:05, 34.3Mit/s]      \n",
            "Fetching model.ckpt-1000000.data-00031-of-00032: 170Mit [00:05, 32.6Mit/s]      \n",
            "Fetching model.ckpt-1000000.index: 13.0kit [00:00, 7.01Mit/s]                   \n",
            "Fetching model.ckpt-1000000.meta: 20.5Mit [00:00, 39.2Mit/s]                    \n",
            "Fetching kmeans_centers.npy: 7.00kit [00:00, 4.65Mit/s]                         \n",
            "input args:\n",
            " {\n",
            "    \"download_dir\":\"/content/clusters\",\n",
            "    \"model\":null,\n",
            "    \"ckpt\":null,\n",
            "    \"clusters\":true,\n",
            "    \"dataset\":null\n",
            "}\n",
            "Fetching kmeans_centers.npy: 7.00kit [00:00, 6.52Mit/s]                         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjVa2W5f-6BB",
        "colab_type": "text"
      },
      "source": [
        "## HuggingFace Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoImkBA2d5jT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "4e28112b-78c2-4697-e6f9-c5e34876fab0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 19.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 26.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=ce2433de68199bfa53dae26d9d46988b2ac237b3b8bb5064e437abdbb027767a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soYP5NE_KPZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import transformers\n",
        "from transformers.modeling_gpt2 import GPT2Model,GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def load_tf_weights_in_image_gpt2(model, config, gpt2_checkpoint_path):\n",
        "    \"\"\" Load tf checkpoints in a pytorch model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import re\n",
        "        import tensorflow as tf\n",
        "    except ImportError:\n",
        "        logger.error(\n",
        "            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n",
        "            \"https://www.tensorflow.org/install/ for installation instructions.\"\n",
        "        )\n",
        "        raise\n",
        "    tf_path = os.path.abspath(gpt2_checkpoint_path)\n",
        "    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n",
        "    # Load weights from TF model\n",
        "    init_vars = tf.train.list_variables(tf_path)\n",
        "    names = []\n",
        "    arrays = []\n",
        "\n",
        "    for name, shape in init_vars:\n",
        "        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
        "        array = tf.train.load_variable(tf_path, name)\n",
        "        names.append(name)\n",
        "        arrays.append(array.squeeze())\n",
        "\n",
        "    for name, array in zip(names, arrays):\n",
        "        name = name[6:]  # skip \"model/\"\n",
        "        name = name.split(\"/\")\n",
        "\n",
        "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
        "        # which are not required for using pretrained model\n",
        "        if any(\n",
        "            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n",
        "            for n in name\n",
        "        ) or name[-1] in ['_step']:\n",
        "            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
        "            continue\n",
        "        \n",
        "        pointer = model\n",
        "        if name[-1] not in [\"wtet\"]:\n",
        "          pointer = getattr(pointer, \"transformer\")\n",
        "        \n",
        "        for m_name in name:\n",
        "            if re.fullmatch(r\"[A-Za-z]+\\d+\", m_name):\n",
        "                scope_names = re.split(r\"(\\d+)\", m_name)\n",
        "            else:\n",
        "                scope_names = [m_name]\n",
        "\n",
        "            if scope_names[0] == \"w\" or scope_names[0] == \"g\":\n",
        "                pointer = getattr(pointer, \"weight\")\n",
        "            elif scope_names[0] == \"b\":\n",
        "                pointer = getattr(pointer, \"bias\")\n",
        "            elif scope_names[0] == \"wpe\" or scope_names[0] == \"wte\":\n",
        "                pointer = getattr(pointer, scope_names[0])\n",
        "                pointer = getattr(pointer, \"weight\")\n",
        "            elif scope_names[0] in ['q_proj','k_proj','v_proj']:\n",
        "                pointer = getattr(pointer, 'c_attn')\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif len(name) ==3 and name[1]==\"attn\" and scope_names[0]==\"c_proj\":\n",
        "                pointer = getattr(pointer, scope_names[0])\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif scope_names[0]==\"wtet\":\n",
        "                pointer = getattr(pointer, \"lm_head\")\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif scope_names[0]==\"sos\":\n",
        "                pointer = getattr(pointer,\"wte\")\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            else:\n",
        "                pointer = getattr(pointer, scope_names[0])\n",
        "            if len(scope_names) >= 2:\n",
        "                num = int(scope_names[1])\n",
        "                pointer = pointer[num]\n",
        "\n",
        "        if len(name) > 1 and name[1]==\"attn\" or name[-1]==\"wtet\" or name[-1]==\"sos\" or name[-1]==\"wte\":\n",
        "           pass #array is used to initialize only part of the pointer so sizes won't match\n",
        "        else:\n",
        "          try:\n",
        "              assert pointer.shape == array.shape\n",
        "          except AssertionError as e:\n",
        "              e.args += (pointer.shape, array.shape)\n",
        "              raise\n",
        "          \n",
        "        logger.info(\"Initialize PyTorch weight {}\".format(name))\n",
        "\n",
        "        if name[-1]==\"q_proj\":\n",
        "          pointer.data[:,:config.n_embd] = torch.from_numpy(array.reshape(config.n_embd,config.n_embd) ).T\n",
        "        elif name[-1]==\"k_proj\":\n",
        "          pointer.data[:,config.n_embd:2*config.n_embd] = torch.from_numpy(array.reshape(config.n_embd,config.n_embd) ).T\n",
        "        elif name[-1]==\"v_proj\":\n",
        "          pointer.data[:,2*config.n_embd:] = torch.from_numpy(array.reshape(config.n_embd,config.n_embd) ).T\n",
        "        elif (len(name) ==3 and name[1]==\"attn\" and name[2]==\"c_proj\" ):\n",
        "          pointer.data = torch.from_numpy(array.reshape(config.n_embd,config.n_embd) )\n",
        "        elif name[-1]==\"wtet\":\n",
        "          pointer.data = torch.from_numpy(array)\n",
        "        elif name[-1]==\"wte\":\n",
        "          pointer.data[:config.vocab_size-1,:] = torch.from_numpy(array)\n",
        "        elif name[-1]==\"sos\":\n",
        "          pointer.data[-1] = torch.from_numpy(array)\n",
        "        else:\n",
        "          pointer.data = torch.from_numpy(array)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "class ln_mod(nn.Module):\n",
        "    def __init__(self, nx,eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = Parameter(torch.Tensor(nx))\n",
        "    def forward(self,x):#input is not mean centered\n",
        "        return x / torch.sqrt( torch.std(x,axis=-1,unbiased=False,keepdim=True)**2 + self.eps ) * self.weight.data[...,:] \n",
        "\n",
        "def replace_ln(m, name,config):\n",
        "  for attr_str in dir(m):\n",
        "      target_attr = getattr(m, attr_str)\n",
        "      if type(target_attr) == torch.nn.LayerNorm:\n",
        "          #print('replaced: ', name, attr_str)\n",
        "          setattr(m, attr_str, ln_mod(config.n_embd,config.layer_norm_epsilon))\n",
        "\n",
        "  for n, ch in m.named_children():\n",
        "      replace_ln(ch, n,config)        \n",
        "\n",
        "def gelu2(x):\n",
        "    return x * torch.sigmoid(1.702 * x)\n",
        "\n",
        "class ImageGPT2LMHeadModel(GPT2LMHeadModel):\n",
        "  load_tf_weights = load_tf_weights_in_image_gpt2\n",
        "  \n",
        "  def __init__(self, config):\n",
        "      super().__init__(config)\n",
        "      self.lm_head = nn.Linear(config.n_embd, config.vocab_size - 1, bias=False)\n",
        "      replace_ln(self,\"net\",config) #replace layer normalization\n",
        "      for n in range(config.n_layer):\n",
        "        self.transformer.h[n].mlp.act = gelu2 #replace activation \n",
        "\n",
        "  def tie_weights(self): #image-gpt doesn't tie output and input embeddings\n",
        "    pass "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxJvFFK8gqJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "color_clusters_file = \"%s/kmeans_centers.npy\"%(color_clusters_dir)\n",
        "clusters = np.load(color_clusters_file) #get color clusters"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q00XounhcEIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODELS={\"l\":(1536,16,48),\"m\":(1024,8,36),\"s\":(512,8,24) } \n",
        "n_embd,n_head,n_layer=MODELS[model_size] #set model hyperparameters\n",
        "vocab_size = len(clusters) + 1 #add one for start of sentence token\n",
        "config = transformers.GPT2Config(vocab_size=vocab_size,n_ctx=n_px*n_px,n_positions=n_px*n_px,n_embd=n_embd,n_layer=n_layer,n_head=n_head)\n",
        "model_path = \"%s/%s/model.ckpt-1000000.index\"%(models_dir,model_size)\n",
        "\n",
        "model = ImageGPT2LMHeadModel.from_pretrained(model_path,from_tf=True,config=config).cuda()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntzYL6JVkIsg",
        "colab_type": "text"
      },
      "source": [
        "## Image Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KU6HMyXkHsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Resize original images to n_px by n_px\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#numpy implementation of functions in image-gpt/src/utils which convert pixels of image to nearest color cluster. \n",
        "def normalize_img(img):\n",
        "  return img/127.5 - 1\n",
        "\n",
        "def squared_euclidean_distance_np(a,b):\n",
        "  b = b.T\n",
        "  a2 = np.sum(np.square(a),axis=1)\n",
        "  b2 = np.sum(np.square(b),axis=0)\n",
        "  ab = np.matmul(a,b)\n",
        "  d = a2[:,None] - 2*ab + b2[None,:]\n",
        "  return d\n",
        "\n",
        "def color_quantize_np(x, clusters):\n",
        "    x = x.reshape(-1, 3)\n",
        "    d = squared_euclidean_distance_np(x, clusters)\n",
        "    return np.argmin(d,axis=1)\n",
        "\n",
        "def resize(image_paths, rotate_90=False):\n",
        "  dim=(n_px,n_px)\n",
        "  x = np.zeros((len(image_paths),n_px,n_px,3),dtype=np.uint8)\n",
        "\n",
        "  for n,image_path in enumerate(image_paths):\n",
        "    img_np = cv2.imread(image_path)   # reads an image in the BGR format\n",
        "    img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)   # BGR -> RGB\n",
        "    H,W,C = img_np.shape\n",
        "    D = min(H,W)\n",
        "    img_np = img_np[:D,:D,:C] #get square piece of image\n",
        "    if (rotate_90):\n",
        "      img_np = cv2.rotate(img_np, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
        "    x[n] = cv2.resize(img_np,dim, interpolation = cv2.INTER_AREA) #resize to n_px by n_px\n",
        "\n",
        "  return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-kUzrTceiwk",
        "colab_type": "text"
      },
      "source": [
        "## Embedding Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsBZ_keDi9Kq",
        "colab_type": "text"
      },
      "source": [
        "Prior implementations:\n",
        "- SENT (w/ jiant) - https://github.com/W4ngatang/sent-bias\n",
        "- Tan & Celis (w/ HuggingFace) - https://github.com/tanyichern/social-biases-contextualized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_katnIhyZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9bca2752-36c1-45f6-eb8d-28b2469c3ba3"
      },
      "source": [
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/hbottle.jpg > bottle.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/hcamera.jpg > camera.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/hcoke.jpg > coke.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/hice_cream.jpg > icecream.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/hphone.jpg > phone.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/hwalkman.jpg > walkman.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/hwallet.jpg > wallet.jpg"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5201  100  5201    0     0   5216      0 --:--:-- --:--:-- --:--:--  5211\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  7363  100  7363    0     0   7333      0  0:00:01  0:00:01 --:--:--  7340\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49y_k2iAk5nm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "f7f4f2ae-2cfc-45d9-ddaf-5a9ca72eb267"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_paths = [\"bottle.jpg\", \"camera.jpg\", \"coke.jpg\", \"icecream.jpg\", \"phone.jpg\", \"walkman.jpg\", \"wallet.jpg\"]\n",
        "x = resize(image_paths)\n",
        "x_norm = normalize_img(x) #normalize pixels values to -1 to +1\n",
        "samples = color_quantize_np(x_norm,clusters).reshape(x_norm.shape[:-1]) #map pixels to closest color cluster\n",
        "samples_img = [np.reshape(np.rint(127.5 * (clusters[s] + 1.0)), [n_px, n_px, 3]).astype(np.uint8) for s in samples] # convert color clusters back to pixels\n",
        "f, axes = plt.subplots(1,len(image_paths),dpi=300)\n",
        "for img, ax in zip(samples_img, axes):\n",
        "  ax.axis('off')\n",
        "  ax.imshow(img)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x1200 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87u-VdmBlGCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}