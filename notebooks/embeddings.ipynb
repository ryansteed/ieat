{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image-GPT-Bias-HuggingFace.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9tObiU-qVgv",
        "colab_type": "text"
      },
      "source": [
        "#Image GPT Bias Analysis w/ Hugging Face\n",
        "Ryan Steed\n",
        "\n",
        "Adapted from https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Transformers_Image_GPT.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoWYiUzT_lLs",
        "colab_type": "text"
      },
      "source": [
        "## Download Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzJbEWC6Vogc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1a66b954-bd5b-4f84-fa49-f29f6e2f6a12"
      },
      "source": [
        "!nvidia-smi #OpenAI says you need 16GB GPU for the large model, but it may work if you lower n_sub_batch on the others."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkPOYJsCTaUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4da0a28e-af38-4e72-a08b-3ffcff0f29a9"
      },
      "source": [
        "!git clone https://github.com/openai/image-gpt.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'image-gpt'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Total 41 (delta 0), reused 0 (delta 0), pack-reused 41\u001b[K\n",
            "Unpacking objects: 100% (41/41), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0DcaUYv8LYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_sizes = [\"s\", \"m\", \"l\"] #small medium large, xl not available\n",
        "model_size = \"s\"\n",
        "models_dir = \"/content/models\"\n",
        "color_clusters_dir = \"/content/clusters\"\n",
        "n_px = 32"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylcjIJcwXsFw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "01090b6e-d1f1-4ea6-c94e-1ac6ae919487"
      },
      "source": [
        "!python image-gpt/download.py --model {model_size} --ckpt 1000000 --clusters --download_dir {models_dir}/{model_size}\n",
        "!python image-gpt/download.py --clusters --download_dir {color_clusters_dir}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input args:\n",
            " {\n",
            "    \"download_dir\":\"/content/models/s\",\n",
            "    \"model\":\"s\",\n",
            "    \"ckpt\":\"1000000\",\n",
            "    \"clusters\":true,\n",
            "    \"dataset\":null\n",
            "}\n",
            "Fetching model.ckpt-1000000.data-00000-of-00032: 1.00kit [00:00, 687kit/s]      \n",
            "Fetching model.ckpt-1000000.data-00001-of-00032: 31.5Mit [00:00, 67.4Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00002-of-00032: 28.3Mit [00:00, 65.8Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00003-of-00032: 28.3Mit [00:00, 68.4Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00004-of-00032: 28.3Mit [00:00, 64.0Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00005-of-00032: 31.5Mit [00:00, 56.9Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00006-of-00032: 30.4Mit [00:00, 57.9Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00007-of-00032: 28.3Mit [00:00, 70.5Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00008-of-00032: 31.5Mit [00:00, 66.8Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00009-of-00032: 29.4Mit [00:00, 58.2Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00010-of-00032: 31.5Mit [00:00, 62.2Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00011-of-00032: 31.5Mit [00:00, 40.3Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00012-of-00032: 30.4Mit [00:00, 59.0Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00013-of-00032: 28.3Mit [00:00, 59.1Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00014-of-00032: 31.5Mit [00:00, 74.2Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00015-of-00032: 32.5Mit [00:00, 68.6Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00016-of-00032: 31.5Mit [00:00, 64.3Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00017-of-00032: 28.3Mit [00:00, 64.8Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00018-of-00032: 29.4Mit [00:00, 78.1Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00019-of-00032: 28.3Mit [00:00, 49.4Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00020-of-00032: 28.3Mit [00:00, 57.3Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00021-of-00032: 28.3Mit [00:00, 57.8Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00022-of-00032: 30.4Mit [00:00, 60.1Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00023-of-00032: 30.4Mit [00:00, 62.0Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00024-of-00032: 29.4Mit [00:00, 68.5Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00025-of-00032: 28.3Mit [00:00, 63.2Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00026-of-00032: 28.3Mit [00:00, 62.3Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00027-of-00032: 29.4Mit [00:00, 62.8Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00028-of-00032: 29.4Mit [00:00, 56.2Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00029-of-00032: 29.4Mit [00:00, 59.1Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00030-of-00032: 28.3Mit [00:00, 56.8Mit/s]     \n",
            "Fetching model.ckpt-1000000.data-00031-of-00032: 28.3Mit [00:00, 60.5Mit/s]     \n",
            "Fetching model.ckpt-1000000.index: 10.0kit [00:00, 7.28Mit/s]                   \n",
            "Fetching model.ckpt-1000000.meta: 18.4Mit [00:00, 53.5Mit/s]                    \n",
            "Fetching kmeans_centers.npy: 7.00kit [00:00, 4.94Mit/s]                         \n",
            "input args:\n",
            " {\n",
            "    \"download_dir\":\"/content/clusters\",\n",
            "    \"model\":null,\n",
            "    \"ckpt\":null,\n",
            "    \"clusters\":true,\n",
            "    \"dataset\":null\n",
            "}\n",
            "Fetching kmeans_centers.npy: 7.00kit [00:00, 4.53Mit/s]                         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjVa2W5f-6BB",
        "colab_type": "text"
      },
      "source": [
        "## HuggingFace Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoImkBA2d5jT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "f8c8b8c0-3628-45fd-d9eb-a3a2828fede6"
      },
      "source": [
        "import os\n",
        "\n",
        "!pip install transformers\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 2.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 14.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 33.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=7e904f44478b4806abae0dbe6c872b12da918d69b81d824145eb95d8a511399e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soYP5NE_KPZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import transformers\n",
        "from transformers.modeling_gpt2 import GPT2Model,GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def load_tf_weights_in_image_gpt2(model, config, gpt2_checkpoint_path):\n",
        "    \"\"\" Load tf checkpoints in a pytorch model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import re\n",
        "        import tensorflow as tf\n",
        "    except ImportError:\n",
        "        logger.error(\n",
        "            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n",
        "            \"https://www.tensorflow.org/install/ for installation instructions.\"\n",
        "        )\n",
        "        raise\n",
        "    tf_path = os.path.abspath(gpt2_checkpoint_path)\n",
        "    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n",
        "    # Load weights from TF model\n",
        "    init_vars = tf.train.list_variables(tf_path)\n",
        "    names = []\n",
        "    arrays = []\n",
        "\n",
        "    for name, shape in init_vars:\n",
        "        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
        "        array = tf.train.load_variable(tf_path, name)\n",
        "        names.append(name)\n",
        "        arrays.append(array.squeeze())\n",
        "\n",
        "    for name, array in zip(names, arrays):\n",
        "        name = name[6:]  # skip \"model/\"\n",
        "        name = name.split(\"/\")\n",
        "\n",
        "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
        "        # which are not required for using pretrained model\n",
        "        if any(\n",
        "            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n",
        "            for n in name\n",
        "        ) or name[-1] in ['_step']:\n",
        "            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
        "            continue\n",
        "        \n",
        "        pointer = model\n",
        "        if name[-1] not in [\"wtet\"]:\n",
        "          pointer = getattr(pointer, \"transformer\")\n",
        "        \n",
        "        for m_name in name:\n",
        "            if re.fullmatch(r\"[A-Za-z]+\\d+\", m_name):\n",
        "                scope_names = re.split(r\"(\\d+)\", m_name)\n",
        "            else:\n",
        "                scope_names = [m_name]\n",
        "\n",
        "            if scope_names[0] == \"w\" or scope_names[0] == \"g\":\n",
        "                pointer = getattr(pointer, \"weight\")\n",
        "            elif scope_names[0] == \"b\":\n",
        "                pointer = getattr(pointer, \"bias\")\n",
        "            elif scope_names[0] == \"wpe\" or scope_names[0] == \"wte\":\n",
        "                pointer = getattr(pointer, scope_names[0])\n",
        "                pointer = getattr(pointer, \"weight\")\n",
        "            elif scope_names[0] in ['q_proj','k_proj','v_proj']:\n",
        "                pointer = getattr(pointer, 'c_attn')\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif len(name) ==3 and name[1]==\"attn\" and scope_names[0]==\"c_proj\":\n",
        "                pointer = getattr(pointer, scope_names[0])\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif scope_names[0]==\"wtet\":\n",
        "                pointer = getattr(pointer, \"lm_head\")\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif scope_names[0]==\"sos\":\n",
        "                pointer = getattr(pointer,\"wte\")\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            else:\n",
        "                pointer = getattr(pointer, scope_names[0])\n",
        "            if len(scope_names) >= 2:\n",
        "                num = int(scope_names[1])\n",
        "                pointer = pointer[num]\n",
        "\n",
        "        if len(name) > 1 and name[1]==\"attn\" or name[-1]==\"wtet\" or name[-1]==\"sos\" or name[-1]==\"wte\":\n",
        "           pass #array is used to initialize only part of the pointer so sizes won't match\n",
        "        else:\n",
        "          try:\n",
        "              assert pointer.shape == array.shape\n",
        "          except AssertionError as e:\n",
        "              e.args += (pointer.shape, array.shape)\n",
        "              raise\n",
        "          \n",
        "        logger.info(\"Initialize PyTorch weight {}\".format(name))\n",
        "\n",
        "        if name[-1]==\"q_proj\":\n",
        "          pointer.data[:,:config.n_embd] = torch.from_numpy(array.reshape(config.n_embd,config.n_embd) ).T\n",
        "        elif name[-1]==\"k_proj\":\n",
        "          pointer.data[:,config.n_embd:2*config.n_embd] = torch.from_numpy(array.reshape(config.n_embd,config.n_embd) ).T\n",
        "        elif name[-1]==\"v_proj\":\n",
        "          pointer.data[:,2*config.n_embd:] = torch.from_numpy(array.reshape(config.n_embd,config.n_embd) ).T\n",
        "        elif (len(name) ==3 and name[1]==\"attn\" and name[2]==\"c_proj\" ):\n",
        "          pointer.data = torch.from_numpy(array.reshape(config.n_embd,config.n_embd) )\n",
        "        elif name[-1]==\"wtet\":\n",
        "          pointer.data = torch.from_numpy(array)\n",
        "        elif name[-1]==\"wte\":\n",
        "          pointer.data[:config.vocab_size-1,:] = torch.from_numpy(array)\n",
        "        elif name[-1]==\"sos\":\n",
        "          pointer.data[-1] = torch.from_numpy(array)\n",
        "        else:\n",
        "          pointer.data = torch.from_numpy(array)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "class ln_mod(nn.Module):\n",
        "    def __init__(self, nx,eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = Parameter(torch.Tensor(nx))\n",
        "    def forward(self,x):#input is not mean centered\n",
        "        return x / torch.sqrt( torch.std(x,axis=-1,unbiased=False,keepdim=True)**2 + self.eps ) * self.weight.data[...,:] \n",
        "\n",
        "def replace_ln(m, name,config):\n",
        "  for attr_str in dir(m):\n",
        "      target_attr = getattr(m, attr_str)\n",
        "      if type(target_attr) == torch.nn.LayerNorm:\n",
        "          #print('replaced: ', name, attr_str)\n",
        "          setattr(m, attr_str, ln_mod(config.n_embd,config.layer_norm_epsilon))\n",
        "\n",
        "  for n, ch in m.named_children():\n",
        "      replace_ln(ch, n,config)        \n",
        "\n",
        "def gelu2(x):\n",
        "    return x * torch.sigmoid(1.702 * x)\n",
        "\n",
        "class ImageGPT2LMHeadModel(GPT2LMHeadModel):\n",
        "  load_tf_weights = load_tf_weights_in_image_gpt2\n",
        "  \n",
        "  def __init__(self, config):\n",
        "      super().__init__(config)\n",
        "      self.lm_head = nn.Linear(config.n_embd, config.vocab_size - 1, bias=False)\n",
        "      replace_ln(self,\"net\",config) #replace layer normalization\n",
        "      for n in range(config.n_layer):\n",
        "        self.transformer.h[n].mlp.act = gelu2 #replace activation \n",
        "\n",
        "  def tie_weights(self): #image-gpt doesn't tie output and input embeddings\n",
        "    pass "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxJvFFK8gqJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "color_clusters_file = \"%s/kmeans_centers.npy\"%(color_clusters_dir)\n",
        "clusters = np.load(color_clusters_file) #get color clusters"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q00XounhcEIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODELS={\"l\":(1536,16,48),\"m\":(1024,8,36),\"s\":(512,8,24) } \n",
        "n_embd,n_head,n_layer=MODELS[model_size] #set model hyperparameters\n",
        "vocab_size = len(clusters) + 1 #add one for start of sentence token\n",
        "config = transformers.GPT2Config(vocab_size=vocab_size,n_ctx=n_px*n_px,n_positions=n_px*n_px,n_embd=n_embd,n_layer=n_layer,n_head=n_head)\n",
        "model_path = \"%s/%s/model.ckpt-1000000.index\"%(models_dir,model_size)\n",
        "\n",
        "model = ImageGPT2LMHeadModel.from_pretrained(model_path,from_tf=True,config=config)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntzYL6JVkIsg",
        "colab_type": "text"
      },
      "source": [
        "## Image Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KU6HMyXkHsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Resize original images to n_px by n_px\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#numpy implementation of functions in image-gpt/src/utils which convert pixels of image to nearest color cluster. \n",
        "def normalize_img(img):\n",
        "  return img/127.5 - 1\n",
        "\n",
        "def squared_euclidean_distance_np(a,b):\n",
        "  b = b.T\n",
        "  a2 = np.sum(np.square(a),axis=1)\n",
        "  b2 = np.sum(np.square(b),axis=0)\n",
        "  ab = np.matmul(a,b)\n",
        "  d = a2[:,None] - 2*ab + b2[None,:]\n",
        "  return d\n",
        "\n",
        "def color_quantize_np(x, clusters):\n",
        "    x = x.reshape(-1, 3)\n",
        "    d = squared_euclidean_distance_np(x, clusters)\n",
        "    return np.argmin(d,axis=1)\n",
        "\n",
        "def resize(image_paths, rotate_90=False):\n",
        "  dim=(n_px,n_px)\n",
        "  x = np.zeros((len(image_paths),n_px,n_px,3),dtype=np.uint8)\n",
        "\n",
        "  for n,image_path in enumerate(image_paths):\n",
        "    img_np = cv2.imread(image_path)   # reads an image in the BGR format\n",
        "    img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)   # BGR -> RGB\n",
        "    H,W,C = img_np.shape\n",
        "    D = min(H,W)\n",
        "    img_np = img_np[:D,:D,:C] #get square piece of image\n",
        "    if (rotate_90):\n",
        "      img_np = cv2.rotate(img_np, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
        "    x[n] = cv2.resize(img_np,dim, interpolation = cv2.INTER_AREA) #resize to n_px by n_px\n",
        "\n",
        "  return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-kUzrTceiwk",
        "colab_type": "text"
      },
      "source": [
        "## Embedding Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsBZ_keDi9Kq",
        "colab_type": "text"
      },
      "source": [
        "Prior implementations:\n",
        "- SENT (w/ jiant) - https://github.com/W4ngatang/sent-bias\n",
        "- Tan & Celis (w/ HuggingFace) - https://github.com/tanyichern/social-biases-contextualized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_katnIhyZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "71e4051c-b957-464f-fdd2-8c1946a21b0a"
      },
      "source": [
        "# a couple example sets\n",
        "## tools\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/hbottle.jpg > bottle.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/hcamera.jpg > camera.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/hcoke.jpg > coke.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/hice_cream.jpg > icecream.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/hphone.jpg > phone.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/hwalkman.jpg > walkman.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/hwallet.jpg > wallet.jpg\n",
        "tools = [\"bottle.jpg\", \"camera.jpg\", \"coke.jpg\", \"icecream.jpg\", \"phone.jpg\", \"walkman.jpg\", \"wallet.jpg\"]\n",
        "## weapons\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/waxe.jpg > axe.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/wcannon.jpg > cannon.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/wgrenade.jpg > grenade.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/wmace.jpg > mace.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/wrevolver.jpg > revolver.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/wrifle.jpg > rifle.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/wsword.jpg > sword.jpg\n",
        "weapons = [\"axe.jpg\", \"cannon.jpg\", \"grenade.jpg\", \"mace.jpg\", \"revolver.jpg\", \"rifle.jpg\", \"sword.jpg\"]\n",
        "## black people\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/bf14.jpg > bf14.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/bf23.jpg > bf23.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/bf56.jpg > bf56.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/bm14.jpg > bm14.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/bm23.jpg > bm23.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/bm56.jpg > bm56.jpg\n",
        "black_people = [\"bf14.jpg\", \"bf23.jpg\", \"bf56.jpg\", \"bm14.jpg\", \"bm23.jpg\", \"bm56.jpg\"]\n",
        "\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/wf2.jpg > wf2.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/wf3.jpg > wf3.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/wf6.jpg > wf6.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/wm1.jpg > wm1.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/wm4.jpg > wm4.jpg\n",
        "!curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/wm6.jpg > wm6.jpg\n",
        "white_people = [\"wf2.jpg\", \"wf3.jpg\", \"wf6.jpg\", \"wm1.jpg\", \"wm4.jpg\", \"wm6.jpg\"]\n",
        "\n",
        "image_paths = tools + weapons + black_people + white_people"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5201  100  5201    0     0  40007      0 --:--:-- --:--:-- --:--:-- 40007\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  7363  100  7363    0     0  57077      0 --:--:-- --:--:-- --:--:-- 57077\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  6533  100  6533    0     0  89493      0 --:--:-- --:--:-- --:--:-- 89493\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  6358  100  6358    0     0  48534      0 --:--:-- --:--:-- --:--:-- 48534\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  7238  100  7238    0     0  74618      0 --:--:-- --:--:-- --:--:-- 74618\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  6688  100  6688    0     0  91616      0 --:--:-- --:--:-- --:--:-- 91616\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  6563  100  6563    0     0  91152      0 --:--:-- --:--:-- --:--:-- 91152\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4258  100  4258    0     0  59138      0 --:--:-- --:--:-- --:--:-- 59138\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  7165  100  7165    0     0  76223      0 --:--:-- --:--:-- --:--:-- 76223\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  7080  100  7080    0     0    98k      0 --:--:-- --:--:-- --:--:--   98k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5024  100  5024    0     0  39559      0 --:--:-- --:--:-- --:--:-- 39559\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4961  100  4961    0     0  68902      0 --:--:-- --:--:-- --:--:-- 67958\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5118  100  5118    0     0  39984      0 --:--:-- --:--:-- --:--:-- 39984\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3702  100  3702    0     0  50027      0 --:--:-- --:--:-- --:--:-- 49360\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  8610  100  8610    0     0   116k      0 --:--:-- --:--:-- --:--:--  116k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  8931  100  8931    0     0   111k      0 --:--:-- --:--:-- --:--:--  110k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  8292  100  8292    0     0   112k      0 --:--:-- --:--:-- --:--:--  112k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  8490  100  8490    0     0   115k      0 --:--:-- --:--:-- --:--:--  115k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  8680  100  8680    0     0   122k      0 --:--:-- --:--:-- --:--:--  122k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  8637  100  8637    0     0   120k      0 --:--:-- --:--:-- --:--:--  120k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-f43025d44fb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'curl https://implicit.harvard.edu/implicit/user/demo.us/demo.weapons.0003/images/bm56.jpg > bm56.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mblack_people\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"bf14.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bf23.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bf56.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bm14.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bm23.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bm56.jpg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mimage_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtools\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweapons\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mblack_people\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwhite_people\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'white_people' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49y_k2iAk5nm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "b5b01cdf-e897-4e73-fed7-7615a505ba74"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = resize(image_paths)\n",
        "x_norm = normalize_img(x) #normalize pixels values to -1 to +1\n",
        "samples = color_quantize_np(x_norm,clusters).reshape(x_norm.shape[:-1]) #map pixels to closest color cluster\n",
        "samples_img = [np.reshape(np.rint(127.5 * (clusters[s] + 1.0)), [n_px, n_px, 3]).astype(np.uint8) for s in samples] # convert color clusters back to pixels\n",
        "f, axes = plt.subplots(1,len(image_paths),dpi=300)\n",
        "for img, ax in zip(samples_img, axes):\n",
        "  ax.axis('off')\n",
        "  ax.imshow(img)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x1200 with 7 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87u-VdmBlGCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad(): # saves some memory\n",
        "  # initialize with SOS token\n",
        "  context = np.concatenate( \n",
        "      (\n",
        "          np.full( (len(image_paths), 1), vocab_size - 1 ),\n",
        "          samples.reshape(-1,n_px*n_px),\n",
        "      ), axis=1 \n",
        "  )\n",
        "  # DEBUG THIS LATER\n",
        "  # must drop the last pixel to make room for the SOS\n",
        "  context = torch.tensor(context[:,:-1])\n",
        "  enc, _ = model(context)\n",
        "  enc_last = enc[:, -1, :].numpy()  # extract the rep of the last input, as in sent-bias\n",
        "  np.savetxt(\"encs.csv\", enc_last, delimiter=',')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SHnjFRrWj5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# add the image names to the CSV file\n",
        "df = pd.read_csv(\"encs.csv\", header=None)\n",
        "df[\"img\"] = image_paths\n",
        "df.to_csv(\"encs_labeled.csv\")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cUsFIPuTjxL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "2ae17d50-9fec-450e-89a1-2914da27bb98"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"encs_labeled.csv\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_33992c3c-9e7e-4f4b-b595-0705d1be29aa\", \"encs_labeled.csv\", 67933)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF-vbLVkWSLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}